{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d760f8c1-7b3f-4017-a3b5-b1f3a5c3ce67",
   "metadata": {},
   "source": [
    "<a id=\"Intro\"></a>\n",
    "## LLama3 \n",
    "    \n",
    "    1. This is purely to understand the innards of Llama3\n",
    "    2. Build the components for Llama3 with only torch tensors\n",
    "        Implement forward, backward and update for the params\n",
    "    3. Compare the loss, logits of this custom model, with a gpt2 model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc69dfe-a30b-46fb-b1fa-f9a442c8b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avinash/stuff/llama/train_llama3.py:49: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from typing import List\n",
    "import train_llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e312ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=7)\n",
    "TEST = True\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e0167d77-c1b0-41c1-b450-c63a8361c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_print(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if 'debug' in kwargs and kwargs['debug']:\n",
    "            print(\"\\033[94m\" + \"*\" * 50 + \"\\033[0m\")\n",
    "            func(*args, **kwargs)\n",
    "            print(\"\\033[94m\" + \"*\" * 50 + \"\\033[0m\")\n",
    "    return wrapper\n",
    "\n",
    "@decorate_print\n",
    "def cprint(*args, debug=False):\n",
    "    print(\"\\t, \".join(str(arg) for arg in args))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab7ee9-0907-4ce6-9bf7-9d0a63945c3b",
   "metadata": {},
   "source": [
    "#### Small examples to understand requires_grad functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ccab93c-65a0-44db-bc0a-b6a4ed0fbe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no graph to back prop !!!!\n",
      "element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "This will work. y.grad will have a value\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Cannot backprop into a node that does not require grad\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]]) True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18277/2475171799.py:43: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(x.grad, y.grad is None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3., 3.],\n",
       "         [3., 3., 3.]]),\n",
       " tensor([[1.5000000, 1.5000000, 1.5000000],\n",
       "         [1.5000000, 1.5000000, 1.5000000]]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    x, y,z\n",
    "    No path tracking\n",
    "\"\"\"\n",
    "\n",
    "x = torch.ones((2, 3)).float()\n",
    "y = x**2\n",
    "z = y.sum()\n",
    "\n",
    "do = torch.ones_like(z)\n",
    "try:\n",
    "    z.backward(do)\n",
    "except Exception as e:\n",
    "    print(\"There is no graph to back prop !!!!\")\n",
    "    print(e)\n",
    "\n",
    "\"\"\"\n",
    "    x, y(with grad) -> z\n",
    "\"\"\"\n",
    "\n",
    "y.requires_grad = True\n",
    "z = y.sum()\n",
    "try:\n",
    "    z.backward()\n",
    "    print(\"\\nThis will work. y.grad will have a value\")\n",
    "    print(y.grad)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    y.backward(torch.ones_like(y), inputs=[x])\n",
    "except Exception as e:\n",
    "    print(\"\\nCannot backprop into a node that does not require grad\")\n",
    "\n",
    "\"\"\"\n",
    "    x(with grad) -> y(with grad : does not retain grad since it is an intermediate node) -> z\n",
    "\"\"\"\n",
    "x = torch.ones((2, 3), requires_grad=True).float()\n",
    "y = x**2\n",
    "z = y.sum()\n",
    "do = torch.ones_like(z) + 0.5\n",
    "z.backward(do, inputs=[x])\n",
    "print(x.grad, y.grad is None)\n",
    "\n",
    "\"\"\"\n",
    "    x(with grad) -> y(with grad : does not retain grad since it is an intermediate node. Must explicitl retain grad) -> z\n",
    "\"\"\"\n",
    "x = torch.ones((2, 3), requires_grad=True).float()\n",
    "y = x**2\n",
    "z = y.sum()\n",
    "y.retain_grad()\n",
    "do = torch.ones_like(z) + 0.5\n",
    "z.backward(do)\n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263addb7-2be0-43ec-9409-5396e30975a9",
   "metadata": {},
   "source": [
    "## LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6556d908-ec59-4978-8c58-fbfdac302821",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    version: str = \"3.1\"\n",
    "    block_size: int = 10\n",
    "    vocab_size: int = 20\n",
    "    n_layer: int = 8\n",
    "    n_head: int = 8\n",
    "    n_kv_head: int = 2\n",
    "    n_embd: int = 32\n",
    "    ffn_dim_multiplier: float = 1.3\n",
    "    multiple_of: int = 32\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000.0\n",
    "    use_scaled_rope: bool = False\n",
    "    max_gen_batch_size: int = 4\n",
    "    use_kv: bool = True\n",
    "    n_rep = None\n",
    "    flash: bool = False  # use flashattention?\n",
    "    T: int = 5  # number of tokens in the forward pass\n",
    "    B: int = 4\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "        assert self.n_kv_head <= self.n_head\n",
    "        assert self.n_head % self.n_kv_head == 0\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        self.n_rep = self.n_head // self.n_kv_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce15c26-7a2a-4428-9516-c8d8d16fb46e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "config0 = LlamaConfig(\n",
    "    block_size=64,\n",
    "    vocab_size=20,\n",
    "    n_layer=0,\n",
    "    n_head=8,\n",
    "    n_kv_head=4,\n",
    "    n_embd=256,\n",
    "    ffn_dim_multiplier=1.3,\n",
    "    multiple_of=64,\n",
    "    T=32,  # number of tokens in the forward pass\n",
    "    B=4,\n",
    ")\n",
    "\n",
    "config1 = LlamaConfig(\n",
    "    block_size=64,\n",
    "    vocab_size=20,\n",
    "    n_layer=1,\n",
    "    n_head=8,\n",
    "    n_kv_head=4,\n",
    "    n_embd=256,\n",
    "    ffn_dim_multiplier=1.3,\n",
    "    multiple_of=64,\n",
    "    T=32,  # number of tokens in the forward pass\n",
    "    B=4,\n",
    ")\n",
    "\n",
    "config2 = LlamaConfig(\n",
    "    block_size=64,\n",
    "    vocab_size=20,\n",
    "    n_layer=2,\n",
    "    n_head=8,\n",
    "    n_kv_head=4,\n",
    "    n_embd=256,\n",
    "    ffn_dim_multiplier=1.3,\n",
    "    multiple_of=64,\n",
    "    T=32,  # number of tokens in the forward pass\n",
    "    B=4,\n",
    ")\n",
    "\n",
    "B, T, C, nh, n_kv_head, OC = 2, 4, 8, 4, 2, 3\n",
    "config3 = LlamaConfig(B=B, T=T, n_embd=C, n_head=nh, n_kv_head=n_kv_head, OC=OC)\n",
    "\n",
    "large_config = LlamaConfig(\n",
    "    block_size=64,\n",
    "    vocab_size=1000,\n",
    "    n_layer=2,\n",
    "    n_head=8,\n",
    "    n_kv_head=4,\n",
    "    n_embd=512,\n",
    "    ffn_dim_multiplier=1.3,\n",
    "    multiple_of=64,\n",
    "    T=32,  # number of tokens in the forward pass\n",
    "    B=4,\n",
    ")\n",
    "\n",
    "configs = [config0, config1, config2, config3, large_config]\n",
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec3118-a87c-4839-b2b8-c26e42a16558",
   "metadata": {},
   "source": [
    "## forward + backward functions\n",
    "    1. forward and backward function for the different modules of llama3\n",
    "        state stores the variables needed for the backward pass\n",
    "    2. Toggle TEST to enable/disable the tests for the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9141c-3b1a-4fe0-b3de-a7d4bcd77797",
   "metadata": {},
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6643da3d-bbcd-4155-8f7b-9b50ef48bc20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def encoder_forward(wte: torch.Tensor, tokens: torch.Tensor, state=[]):\n",
    "    # state: store the vars needed for the backward pass\n",
    "    # wte : V,C\n",
    "    # tokens : B,T (tokens ids in range(V)) -> index into wte\n",
    "    # out : B,T,C\n",
    "    state.extend([tokens, wte.shape])\n",
    "    return wte[tokens]\n",
    "\n",
    "\n",
    "def encoder_back(dout: torch.Tensor, state=[]):\n",
    "    # dout : B,T,C\n",
    "    # dwte : V,C\n",
    "    tokens = state[0].view(-1)  # B,T\n",
    "    dout = dout.view(-1, dout.shape[-1])\n",
    "    dwte = torch.zeros(state[1])\n",
    "\n",
    "    # loop version --- is there a faster way to do this ????\n",
    "    for i, t in enumerate(tokens):\n",
    "        dwte[t] += dout[i]\n",
    "    return dwte\n",
    "\n",
    "\n",
    "if TEST:  # enable/disable running TEST for this component\n",
    "    B, T, C, V = 2, 5, 5, 90\n",
    "    enc = nn.Embedding(V, C)\n",
    "\n",
    "    tokens = torch.randint(V, (B, T))\n",
    "    assert tokens.shape == (B, T)\n",
    "    enco1 = enc(tokens)\n",
    "    state = []\n",
    "    enco2 = encoder_forward(enc.weight, tokens, state)\n",
    "    assert enco1.shape == (B, T, C)\n",
    "    assert torch.allclose(enco1, enco2)\n",
    "    assert torch.allclose(state[0], tokens)\n",
    "\n",
    "    # backward\n",
    "    dout = torch.randn_like(enco2)\n",
    "    enco1.backward(dout, inputs=[enc.weight])\n",
    "\n",
    "    dwte = encoder_back(dout, state=state)\n",
    "    assert torch.allclose(dwte, enc.weight.grad)\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3be80e9-8910-4a4d-aaab-fb02251ba6e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'V' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dwte \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mV\u001b[49m, C)\u001b[38;5;241m.\u001b[39mview(V, C)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# torch.index_select(dwte,0, tokens.view(-1)) = dout\u001b[39;00m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mindex_select(dwte, \u001b[38;5;241m0\u001b[39m, tokens\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m dout\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'V' is not defined"
     ]
    }
   ],
   "source": [
    "dwte = torch.zeros(V, C).view(V, C)\n",
    "# torch.index_select(dwte,0, tokens.view(-1)) = dout\n",
    "torch.index_select(dwte, 0, tokens.view(-1)) + dout.view(-1, C)\n",
    "# dwte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dedec0-f78a-473f-a610-8631f9d1c265",
   "metadata": {},
   "source": [
    "### rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "77cab994-a265-4cd0-955c-4ab99236d37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def rms_forward(norm_eps, inp: torch.Tensor, weight: torch.Tensor, state=[]):\n",
    "    # inp (B, T, dim)\n",
    "    # weight (dim)\n",
    "    norm = inp / torch.sqrt(inp.pow(2).mean(-1, keepdim=True) + norm_eps)  # B,T,dim\n",
    "    state.extend([inp, norm, norm_eps])\n",
    "    return weight * norm\n",
    "\n",
    "\n",
    "def rms_backward(dout: torch.Tensor, weight: torch.Tensor, state=[]):\n",
    "    # dout : B,T,C\n",
    "    # dweight : C\n",
    "    # dinp : B,T,C\n",
    "    B, T, C = dout.shape\n",
    "    inp, norm, norm_eps = state  # B,T,C\n",
    "    rms = torch.sqrt(inp.pow(2).mean(-1, keepdim=True) + norm_eps)\n",
    "    dweight = torch.sum(dout * norm, [0, 1])  # sum over the B,T dims\n",
    "    dinp = torch.zeros_like(inp)\n",
    "\n",
    "    # is there a better way of doing this ???? this is ugly\n",
    "    dinp += dout * weight / rms * (1.0 - norm * norm / C)\n",
    "    inp = inp.view(-1, C)\n",
    "    dinp = dinp.view(-1, C)\n",
    "    dout = dout.view(-1, C)\n",
    "    norm = norm.view(-1, C)\n",
    "    rms = rms.view(-1)\n",
    "    for i in range(C):\n",
    "        for j in range(C):\n",
    "            if j == i:\n",
    "                continue\n",
    "            dinp[:, i] += (\n",
    "                -dout[:, j] * weight[j] * inp[:, i] * inp[:, j] / (rms * rms * rms * C)\n",
    "            )\n",
    "    return dinp.view(B, T, C), dweight\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    B, T, C, V = 2, 5, 6, 9\n",
    "    inp = torch.rand(B, T, C)\n",
    "    inp.requires_grad = True\n",
    "    weight = torch.ones(C)\n",
    "    rms = train_llama3.RMSNorm(C)\n",
    "    o1 = rms(inp)\n",
    "    dout = torch.randn_like(o1)\n",
    "    o1.backward(dout)\n",
    "    inp.detach_()\n",
    "\n",
    "    state = []\n",
    "    o2 = rms_forward(config1.norm_eps, inp, weight, state)\n",
    "    assert torch.allclose(o1, o2, atol=1e-4), (o1, o2)\n",
    "\n",
    "    # backward\n",
    "    dinp, dw1 = rms_backward(dout, weight, state)\n",
    "    assert torch.allclose(\n",
    "        dw1, rms.weight.grad, atol=1e-3\n",
    "    ), f\"{dw1[:5]},{rms.weight.grad[:5]}\"\n",
    "    assert torch.allclose(\n",
    "        dinp, inp.grad, atol=1e-3\n",
    "    ), f\"{dinp.view(-1)[:10]}, {inp.grad.view(-1)[:10]}\"\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ad45a-8235-4235-a72e-ada150ad2479",
   "metadata": {},
   "source": [
    "### matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a40fb123-d479-49de-b24f-4dea8fc1b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def matmul(inp: torch.Tensor, weight: torch.Tensor, state=[]):\n",
    "    assert inp.shape[-1] == weight.shape[-2], (inp.shape, weight.shape)\n",
    "    state.append(inp)\n",
    "    return inp @ weight\n",
    "\n",
    "\n",
    "def matmul_backward(dout: torch.Tensor, weight: torch.Tensor, state=[]):\n",
    "    inp = state[0]\n",
    "    if len(weight.shape) == 2:\n",
    "        B, T, OC = dout.shape\n",
    "        _, _, C = inp.shape\n",
    "        assert weight.shape == (C, OC), (weight.shape, (C, OC))\n",
    "        dweight = (inp.unsqueeze(-1) * dout.unsqueeze(-2)).sum([0, 1])\n",
    "        dinp = dout @ weight.transpose(-2, -1)\n",
    "        return dinp, dweight\n",
    "    elif len(inp.shape) == 4 and len(weight.shape) == 4:\n",
    "        B, nh, T, C = dout.shape\n",
    "        _, _, _, hs = inp.shape\n",
    "        dinp = dout @ weight.transpose(-2, -1)\n",
    "        dweight = inp.view(B * nh, T, hs).transpose(-2, -1) @ dout.view(B * nh, T, C)\n",
    "        return dinp, dweight.view(weight.shape)\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    B, T, C, OC = 12, 5, 6, 900\n",
    "    inp = torch.rand(B, T, C)\n",
    "    inp.requires_grad = True\n",
    "    mmw = nn.Linear(in_features=C, out_features=OC, bias=False)\n",
    "    mm1 = mmw(inp)\n",
    "    mmdout = torch.randn_like(mm1)\n",
    "    mm1.backward(mmdout)\n",
    "\n",
    "    inp1 = inp.detach()\n",
    "    state = []\n",
    "    mm2 = matmul(inp=inp1, weight=mmw.weight.T, state=state)\n",
    "    assert torch.allclose(mm1, mm2, atol=1e-4)\n",
    "\n",
    "    # backward\n",
    "    mmdinp, mmdw = matmul_backward(mmdout, mmw.weight.T, state)\n",
    "\n",
    "    assert torch.allclose(mmdinp, inp.grad, atol=1e-4), (\n",
    "        mmdinp.view(-1)[:5],\n",
    "        inp.grad.view(-1)[:5],\n",
    "    )\n",
    "    assert torch.allclose(mmdw, mmw.weight.grad.T, atol=1e-4), (\n",
    "        mmdw.view(-1)[:5],\n",
    "        mmw.weight.grad.T.reshape(-1)[:5],\n",
    "    )\n",
    "\n",
    "    # backward q.KT\n",
    "    B, T, nh, hs = 2, 3, 4, 5\n",
    "    q = torch.randn((B, nh, T, hs))\n",
    "    q.requires_grad = True\n",
    "    kt = torch.randn((B, nh, hs, T))\n",
    "    kt.requires_grad = True\n",
    "    mmo1 = q @ kt\n",
    "\n",
    "    dout = torch.randn_like(mmo1)\n",
    "    mmo1.backward(dout, inputs=[q, kt])\n",
    "    q.detach_(), kt.detach_()\n",
    "\n",
    "    state = []\n",
    "    mmo2 = matmul(q, kt, state=state)  # state =[q]\n",
    "    assert torch.allclose(mmo1, mmo2)\n",
    "    mmdq, mmdkt = matmul_backward(dout, kt, state)\n",
    "\n",
    "    assert torch.allclose(q.grad, mmdq), (q.grad.shape, mmdq.shape)\n",
    "    assert torch.allclose(kt.grad, mmdkt)\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92fccf5-78ab-461d-b48f-6615a8bf1204",
   "metadata": {},
   "source": [
    "### silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "91be77e0-4cc6-412d-83d8-1d667dfbece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def silu(inp: torch.Tensor, state=[]):\n",
    "    state.append(inp)\n",
    "    return inp / (1.0 + torch.exp(-inp))\n",
    "\n",
    "\n",
    "def silu_backward(dout: torch.Tensor, state=[]):\n",
    "    x = state[0]\n",
    "    expx = torch.exp(-x)\n",
    "    den = 1.0 / (1 + expx)\n",
    "    return dout * den * (1 + x * expx * den)\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    state = []\n",
    "    inp = torch.rand(B, T, C)\n",
    "    inp.requires_grad = True\n",
    "    sin = nn.functional.silu(inp)\n",
    "    sdout = torch.ones_like(inp)\n",
    "    sin.backward(sdout, inputs=[inp])\n",
    "    assert torch.allclose(sin, silu(inp, state=state), atol=1e-4)\n",
    "    inp.detach_()\n",
    "\n",
    "    # backward\n",
    "    sdin = silu_backward(sdout, state=state)\n",
    "    assert torch.allclose(sdin, inp.grad)\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616bdcf5-4460-47f8-936d-15a252e6b582",
   "metadata": {},
   "source": [
    "### feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4941181a-7b0c-427d-8074-d6b241abc0d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def ff(\n",
    "    inp: torch.Tensor, w1: torch.Tensor, w2: torch.tensor, w3: torch.Tensor, state=[]\n",
    "):\n",
    "    h1 = matmul(inp, w1)\n",
    "    h3 = matmul(inp, w3)\n",
    "    h1silu = silu(h1)\n",
    "    h1h3 = h1silu * h3\n",
    "    state.extend([inp, h1, h3, h1silu, h1h3])\n",
    "    return matmul(h1h3, w2)\n",
    "\n",
    "\n",
    "# backward\n",
    "def ff_backward(\n",
    "    dout: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, w3: torch.Tensor, state=[]\n",
    "):\n",
    "    inp, h1, h3, h1silu, h1h3 = state\n",
    "    dh1h3, dw2 = matmul_backward(dout=dout, weight=w2, state=[h1h3])\n",
    "    dh1silu, dh3 = dh1h3 * h3, dh1h3 * h1silu\n",
    "    dh1 = silu_backward(dout=dh1silu, state=[h1])\n",
    "    dinp1, dw1 = matmul_backward(dout=dh1, weight=w1, state=[inp])\n",
    "    dinp2, dw3 = matmul_backward(dout=dh3, weight=w3, state=[inp])\n",
    "    return dinp1 + dinp2, dw1, dw2, dw3\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    print(len(configs))\n",
    "    for cnfg in configs:\n",
    "        print(\"\\nChecking: \", cnfg)\n",
    "        B, T, C = cnfg.B, cnfg.T, cnfg.n_embd\n",
    "        inp = torch.rand(B, T, C)\n",
    "        inp.requires_grad = True\n",
    "        ffn = train_llama3.MLP(cnfg)\n",
    "        ffo1 = ffn.forward(inp)\n",
    "        dout = torch.randn_like(ffo1)\n",
    "        ffo1.backward(dout)\n",
    "        inp1 = inp.detach()\n",
    "\n",
    "        ffw1 = ffn.c_fc2.weight.transpose(0, 1)\n",
    "        ffw2 = ffn.c_proj.weight.transpose(0, 1)\n",
    "        ffw3 = ffn.c_fc.weight.transpose(0, 1)\n",
    "        state = []\n",
    "        ffo2 = ff(inp1, ffw1, ffw2, ffw3, state)\n",
    "        assert torch.allclose(ffo1, ffo2, atol=1e-4)\n",
    "\n",
    "        ffdin, ffdw1, ffdw2, ffdw3 = ff_backward(\n",
    "            dout=dout, w1=ffw1, w2=ffw2, w3=ffw3, state=state\n",
    "        )\n",
    "        assert torch.allclose(ffdin, inp.grad, atol=1e-4)\n",
    "        assert torch.allclose(ffdw1, ffn.c_fc2.weight.grad.T, atol=1e-4)\n",
    "        assert torch.allclose(ffdw3, ffn.c_fc.weight.grad.T, atol=1e-4)\n",
    "        assert torch.allclose(ffdw2, ffn.c_proj.weight.grad.T, atol=1e-4)\n",
    "print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b96377-d90a-447c-ba12-b15e995d6c12",
   "metadata": {},
   "source": [
    "### repeat_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6375408-e9a5-4b4c-92db-cd85ab56d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def repeat_kv(inp: torch.Tensor, n_rep: int, state=[]):\n",
    "    # inp : B,T, n_kv_heads, dim\n",
    "    state.append(inp)\n",
    "    B, T, n_kv_head, dim = inp.shape\n",
    "    if n_rep == 1:\n",
    "        return inp\n",
    "    # return inp[:,:,:, None,:].repeat(1,1,1,n_rep,1).view(B,T,n_kv_head*n_rep,dim)\n",
    "\n",
    "    # use expand instead of repeat. we shud not be allocation new memory\n",
    "    return (\n",
    "        inp[:, :, :, None, :]\n",
    "        .expand(B, T, n_kv_head, n_rep, dim)\n",
    "        .reshape(B, T, n_kv_head * n_rep, dim)\n",
    "    )\n",
    "\n",
    "\n",
    "def repeat_kv_backward(dout: torch.Tensor, state=[]):\n",
    "    # dout : (B,T,n_head, hs)\n",
    "    # dint : (B,T, n_kv_head, hs)\n",
    "    inp = state[0]\n",
    "    B, T, n_kv_head, hs = inp.shape\n",
    "    _, _, n_head, _ = dout.shape\n",
    "    n_rep = n_head // n_kv_head\n",
    "    return dout.view(B, T, n_kv_head, n_rep, hs).sum(-2)\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    for cnfg in configs:\n",
    "        B, T, n_kv_head, n_head, C = (\n",
    "            cnfg.B,\n",
    "            cnfg.T,\n",
    "            cnfg.n_kv_head,\n",
    "            cnfg.n_head,\n",
    "            cnfg.n_embd,\n",
    "        )\n",
    "        n_rep = n_head // n_kv_head\n",
    "        hs = C // n_head\n",
    "        rkvi = torch.randn((B, T, n_kv_head, hs))\n",
    "        rkvi.requires_grad = True\n",
    "        rkvo1 = train_llama3.repeat_kv(rkvi, n_rep)\n",
    "        dout = torch.randn_like(rkvo1)\n",
    "        rkvo1.backward(dout, inputs=[rkvi])\n",
    "        rkvi1 = rkvi.detach()\n",
    "\n",
    "        state = []\n",
    "        rkvo2 = repeat_kv(rkvi1, n_rep, state=state)\n",
    "        assert torch.allclose(rkvo1, rkvo2, atol=1e-5)\n",
    "\n",
    "        # backward\n",
    "        rkvdo2 = repeat_kv_backward(dout, state)\n",
    "        assert  torch.allclose(rkvi.grad, rkvdo2, atol=1e-5)\n",
    "print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7b14f-3824-4483-9a35-0b27dfcbf200",
   "metadata": {},
   "source": [
    "### Rotary Positional Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "42460bbb-fce3-4af0-9cf1-8da65d4009e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def compute_freqs_cis(\n",
    "    dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False\n",
    "):\n",
    "    # end is generally the block size >= T (input seq length)\n",
    "    theta = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    pos = torch.arange(end)[:, None]\n",
    "    rotations = pos * theta\n",
    "    assert rotations.shape == (end, dim // 2)\n",
    "    cos_theta = torch.cos(rotations)\n",
    "    sin_theta = torch.sin(rotations)\n",
    "    return cos_theta, sin_theta\n",
    "\n",
    "\n",
    "def rpe_forward(inp: torch.Tensor, cos_theta, sin_theta):\n",
    "    if len(inp.shape) == 3:\n",
    "        B, T, C = inp.shape\n",
    "    else:\n",
    "        B, T, n, s = inp.shape\n",
    "        cos_theta, sin_theta = cos_theta.unsqueeze(-2), sin_theta.unsqueeze(-2)\n",
    "\n",
    "    cos_theta, sin_theta = cos_theta[:T], sin_theta[:T]\n",
    "    cprint(\"cos: \", cos_theta.view(-1)[-15:], debug=DEBUG)\n",
    "    cprint(\"sin: \", sin_theta.view(-1)[-15:], debug=DEBUG)\n",
    "    x, y = inp[..., ::2], inp[..., 1::2]\n",
    "    final_x = x * cos_theta - y * sin_theta\n",
    "    final_y = x * sin_theta + y * cos_theta\n",
    "    return torch.concat([final_x[..., None], final_y[..., None]], dim=-1).view(\n",
    "        inp.shape\n",
    "    )\n",
    "\n",
    "\n",
    "def rpe_backward(dout: torch.Tensor, cos_theta, sin_theta):\n",
    "    # dinp has the same shape as dout\n",
    "    if len(dout.shape) == 3:\n",
    "        B, T, C = dout.shape\n",
    "    else:\n",
    "        B, T, n, s = dout.shape\n",
    "        cos_theta, sin_theta = cos_theta.unsqueeze(-2), sin_theta.unsqueeze(-2)\n",
    "    cos_theta, sin_theta = cos_theta[:T], sin_theta[:T]\n",
    "    dout_x, dout_y = dout[..., ::2], dout[..., 1::2]\n",
    "    dx = dout_x * cos_theta + dout_y * sin_theta\n",
    "    dy = dout_x * (-sin_theta) + dout_y * cos_theta\n",
    "    return torch.concat([dx[..., None], dy[..., None]], dim=-1).view(dout.shape)\n",
    "\n",
    "\n",
    "if  TEST:\n",
    "    for cnfg in configs:\n",
    "        B, T, C, maxT = cnfg.B, cnfg.T, cnfg.n_embd, cnfg.block_size\n",
    "        cos, sin = compute_freqs_cis(C, maxT, cnfg.rope_theta)\n",
    "        freqs_cis = train_llama3.precompute_freqs_cis(\n",
    "            dim=C, end=T, theta=cnfg.rope_theta, use_scaled=False\n",
    "        )\n",
    "\n",
    "        # --- the 3d case\n",
    "        x = torch.randn(B, T, C).float()\n",
    "        x1 = torch.randn(B, T, C).float()\n",
    "        x.requires_grad = True\n",
    "        xq, _ = train_llama3.apply_rotary_emb(x, x1, freqs_cis)\n",
    "        dout = torch.ones_like(x)\n",
    "        xq.view(B, T, -1).backward(dout, inputs=[x])\n",
    "\n",
    "        x1 = x.detach()\n",
    "        fx = rpe_forward(x1, cos, sin)\n",
    "        dx = rpe_backward(dout, cos, sin)\n",
    "        assert torch.allclose(\n",
    "            xq.view(B, T, -1), fx, atol=1e-4\n",
    "        ), f\"{xq.view(-1)[-10:], fx.view(-1)[-10:]}\"\n",
    "        assert torch.allclose(dx, x.grad, atol=1e-4)\n",
    "\n",
    "        # --- the 4d case\n",
    "        xq = torch.randn(B, T, 14, C)\n",
    "        xq.requires_grad = True\n",
    "        xqr1 = train_llama3.apply_rotary_emb(xq, xq, freqs_cis)[0]\n",
    "        dout = torch.ones_like(xq)\n",
    "        xqr1.backward(dout, inputs=[xq])\n",
    "\n",
    "        xq1 = xq.detach()\n",
    "        xqr2 = rpe_forward(xq1, cos, sin)\n",
    "        dx = rpe_backward(dout, cos, sin)\n",
    "        assert torch.allclose(xqr1, xqr2, atol=1e-4)\n",
    "        assert torch.allclose(dx, xq.grad, atol=1e-4)\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabee1ef-2923-4561-b299-39fa97d328f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e76f904e-6a46-4c24-86a8-5fe14c448ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "def softmax_forward(inp: torch.Tensor, state=[]):\n",
    "    inp = inp.detach().clone().requires_grad_(True)\n",
    "    out = torch.nn.functional.softmax(inp, dim=-1)\n",
    "    assert inp.requires_grad\n",
    "    assert out.requires_grad\n",
    "    state.extend([inp, out])  # inp, out form a graph\n",
    "    return out.detach()\n",
    "\n",
    "\n",
    "def softmax_backward(dout: torch.Tensor, state=[]):\n",
    "    inp, out = state\n",
    "    assert inp.requires_grad\n",
    "    assert out.requires_grad\n",
    "    out.backward(dout, inputs=[inp])\n",
    "    grad = inp.grad.clone()\n",
    "    inp.detach_()\n",
    "    out.detach_()\n",
    "    assert not inp.requires_grad\n",
    "    assert not out.requires_grad\n",
    "    return grad\n",
    "\n",
    "\n",
    "def cross_entropy_forward(logits: torch.tensor, targets: torch.tensor, state=[]):\n",
    "\n",
    "    B, T, V = logits.shape\n",
    "    log_sm = torch.log(softmax_forward(logits, state))\n",
    "    loss_ce = -torch.gather(log_sm, -1, targets.view(B, T, 1)).sum() / (B * T)\n",
    "    state.append(targets)\n",
    "    return loss_ce\n",
    "\n",
    "\n",
    "def cross_entropy_backward(loss, state):\n",
    "    logits, sm, targets = state\n",
    "    B, T, V = logits.shape\n",
    "    d_log_sm = torch.zeros_like(sm)\n",
    "    d_log_sm.view(B * T, -1)[torch.arange(B * T), targets.view(-1)] = -1.0 / (B * T)\n",
    "    d_log_sm /= state[1]\n",
    "    return softmax_backward(d_log_sm, state=state[:2])\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    for cnfg in configs:\n",
    "        print(\"\\nChecking: \", cnfg)\n",
    "        B, T, C = cnfg.B, cnfg.T, cnfg.n_embd\n",
    "        inp = torch.randn((B, T, C))\n",
    "        state = []\n",
    "        out = softmax_forward(inp, state)\n",
    "        for tnsr in state:\n",
    "            assert tnsr.requires_grad\n",
    "\n",
    "        dout = torch.randn_like(out)\n",
    "        dinp = softmax_backward(dout, state=state)\n",
    "        assert dinp.shape == inp.shape\n",
    "\n",
    "        B, T, C = cnfg.B, cnfg.T, cnfg.n_embd\n",
    "        torch.manual_seed(1)\n",
    "        logits_ce = torch.randn(B, T, C).requires_grad_(True)\n",
    "        targets = torch.randint(C, (B, T))\n",
    "        lossx2 = torch.nn.functional.cross_entropy(\n",
    "            logits_ce.view(-1, logits_ce.size(-1)), targets.view(-1), ignore_index=-1\n",
    "        )\n",
    "        lossx2.backward()\n",
    "\n",
    "        logits1 = logits_ce.detach()\n",
    "        state = []\n",
    "        lossx1 = cross_entropy_forward(logits1, targets, state)\n",
    "        cprint(lossx1, lossx2, debug=DEBUG)\n",
    "        dlogits1 = cross_entropy_backward(lossx1, state)\n",
    "        assert torch.allclose(dlogits1, logits_ce.grad), (\n",
    "            dlogits1.view(-1)[:10],\n",
    "            logits_ce.grad.view(-1)[:10],\n",
    "        )\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ec2a2-c7a2-47d5-a357-44bcf5d3649a",
   "metadata": {},
   "source": [
    "### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "98fe62d6-b9de-4b75-a6c0-af225aef1d8a",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\tForward pass matches\n",
      "        Backward pass matches\n",
      "\n",
      "--------\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\tForward pass matches\n",
      "        Backward pass matches\n",
      "\n",
      "--------\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\tForward pass matches\n",
      "        Backward pass matches\n",
      "\n",
      "--------\n",
      "Checking:  LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n",
      "\tForward pass matches\n",
      "        Backward pass matches\n",
      "\n",
      "--------\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\tForward pass matches\n",
      "        Backward pass matches\n",
      "\n",
      "--------\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def attention(\n",
    "    config: LlamaConfig,\n",
    "    inp: torch.Tensor,\n",
    "    wq: torch.Tensor,\n",
    "    wk: torch.Tensor,\n",
    "    wv: torch.Tensor,\n",
    "    wo: torch.Tensor,\n",
    "    cos_theta,\n",
    "    sin_theta,\n",
    "    state=[],\n",
    "):\n",
    "    # inp : (B,T,C)\n",
    "    # wk,wv : (c,kv_dim)\n",
    "    # wq : c,c\n",
    "    # wo : c,c\n",
    "    state.append(inp)\n",
    "    cprint(\"inp_: \", inp.reshape(-1)[-15:], debug=DEBUG)\n",
    "    B, T, nh, n_kv_head, C = (\n",
    "        config.B,\n",
    "        config.T,\n",
    "        config.n_head,\n",
    "        config.n_kv_head,\n",
    "        config.n_embd,\n",
    "    )\n",
    "    hs = C // nh\n",
    "\n",
    "    kv_dim = hs * n_kv_head\n",
    "    n_rep = nh // n_kv_head\n",
    "    q = matmul(inp, wq).view(B, T, nh, hs)\n",
    "    k = matmul(inp, wk).view(B, T, n_kv_head, hs)\n",
    "    v = matmul(inp, wv).view(B, T, n_kv_head, hs)\n",
    "    state.extend([q, k, v])\n",
    "\n",
    "    cprint(\"q_: \", q.reshape(-1)[-15:], debug=DEBUG)\n",
    "    cprint(\"k_: \", k.reshape(-1)[-15:], debug=DEBUG)\n",
    "    cprint(\"v_: \", v.reshape(-1)[-15:], debug=DEBUG)\n",
    "    # rope\n",
    "    q = rpe_forward(q, cos_theta, sin_theta)\n",
    "    k = rpe_forward(k, cos_theta, sin_theta)\n",
    "    state.extend([q, k])  # this is after rope\n",
    "    cprint(\"qrope_: \", q.reshape(-1)[-15:], debug=DEBUG)\n",
    "    cprint(\"krope_: \", k.reshape(-1)[-15:], debug=DEBUG)\n",
    "    k = repeat_kv(inp=k, n_rep=n_rep)  # b,t, n_head, hs\n",
    "    v = repeat_kv(inp=v, n_rep=n_rep)  # b,t, n_head, hs\n",
    "    cprint(\"krep_: \", k.reshape(-1)[-15:], debug=DEBUG)\n",
    "\n",
    "    state.extend([k, v])  # this is the repeated version\n",
    "\n",
    "    # (b,nh,t,c) @ (b,nh, c, t) -> (b, nh, t,t)\n",
    "    qp = q.permute(0, 2, 1, 3).contiguous()  # b,nh, t,hs\n",
    "    kT = k.permute(0, 2, 3, 1).contiguous()  # b,nh,hs,t -> 0,3,1,2\n",
    "    qkT = matmul(qp, kT) / (math.sqrt(hs))\n",
    "\n",
    "    mask = torch.tril(qkT)\n",
    "    qkT = torch.where(mask == 0.0, -torch.inf, qkT)\n",
    "\n",
    "    state.extend([qp, kT])\n",
    "    scores = softmax_forward(qkT, state)  # torch.softmax(qkT, dim=-1) # b, nh, t,t\n",
    "    cprint(\"scores_: \", scores.reshape(-1)[-15:], debug=DEBUG)\n",
    "    assert not scores.requires_grad\n",
    "\n",
    "    vp = v.permute(0, 2, 1, 3).contiguous()\n",
    "    o = matmul(\n",
    "        scores, vp\n",
    "    )  # b, nh, t, hs -> matmul SHOUlD GENERALIZE for any dim, not just for nn.Linear ???\n",
    "    cprint(\"o_: \", o.reshape(-1)[-15:], debug=DEBUG)\n",
    "    assert not o.requires_grad\n",
    "    o = o.permute(0, 2, 1, 3).reshape(B, T, C)  # b, t, c\n",
    "    state.extend([o, vp])\n",
    "    fo = o @ wo\n",
    "    assert not fo.requires_grad\n",
    "    cprint(\"y_: \", fo.reshape(-1)[-15:], debug=DEBUG)\n",
    "    return fo\n",
    "\n",
    "\n",
    "def attention_backward(\n",
    "    dout: torch.Tensor,\n",
    "    wq: torch.Tensor,\n",
    "    wk: torch.Tensor,\n",
    "    wv: torch.Tensor,\n",
    "    wo: torch.Tensor,\n",
    "    cos_theta,\n",
    "    sin_theta,\n",
    "    state=[],\n",
    "):\n",
    "\n",
    "    # dout: (B,T,C)\n",
    "    inp, q, k, v, qr, kr, k_e, v_e, qp, kT, qkT, scores, o, vp = state\n",
    "    for tnsr in [inp, q, k, v, qr, kr, k_e, v_e, qp, kT, o, vp]:\n",
    "        assert not tnsr.requires_grad\n",
    "    assert k.shape == v.shape\n",
    "    B, T, nh, hs = q.shape\n",
    "    _, _, n_kv_head, khs = k.shape\n",
    "    assert hs == khs\n",
    "    assert qkT.shape == (B, nh, T, T)\n",
    "    n_rep = nh // n_kv_head\n",
    "\n",
    "    do, dwo = matmul_backward(dout=dout, weight=wo, state=[o])  # do : b,t,c\n",
    "    assert not do.requires_grad\n",
    "    assert not dwo.requires_grad\n",
    "    do = (\n",
    "        do.view(B, T, nh, hs).permute(0, 2, 1, 3).contiguous().view(B, nh, T, hs)\n",
    "    )  # do : b,nh, t, hs\n",
    "    dscores, dvp = matmul_backward(\n",
    "        dout=do, weight=vp, state=[scores.detach()]\n",
    "    )  # do : b,t,c\n",
    "    assert not dscores.requires_grad\n",
    "    assert not dvp.requires_grad\n",
    "    dqkT = softmax_backward(dscores, state=[qkT, scores])\n",
    "    assert not dqkT.requires_grad\n",
    "    for tnsr in [qkT, scores]:\n",
    "        assert not tnsr.requires_grad\n",
    "\n",
    "    # do we need this ?????? (think)\n",
    "    # mask = torch.tril(qkT)\n",
    "    # dqkT = torch.where(mask==0.0, 0.0, dqkT)\n",
    "    dqp, dkT = matmul_backward(dqkT, kT, state=[qp])\n",
    "    assert not dqp.requires_grad\n",
    "    assert not dkT.requires_grad\n",
    "    dq = dqp.permute(0, 2, 1, 3).contiguous() / (math.sqrt(hs))\n",
    "    dke = dkT.permute(0, 3, 1, 2).contiguous() / (math.sqrt(hs))\n",
    "    dve = dvp.permute(0, 2, 1, 3).contiguous()\n",
    "    dk = repeat_kv_backward(dke, state=[k])\n",
    "    dv = repeat_kv_backward(dve, state=[v])\n",
    "\n",
    "    # rope backward\n",
    "    dq = rpe_backward(dq, cos_theta, sin_theta)\n",
    "    dk = rpe_backward(dk, cos_theta, sin_theta)\n",
    "    dq = dq.view(B, T, hs * nh)\n",
    "    dk = dk.view(B, T, hs * n_kv_head)\n",
    "    dv = dv.view(B, T, hs * n_kv_head)\n",
    "    for tnsr in [dq, dk, dv]:\n",
    "        assert not tnsr.requires_grad\n",
    "    dinq, dwq = matmul_backward(dout=dq, weight=wq, state=[inp])  # do : b,t,c\n",
    "    dink, dwk = matmul_backward(dout=dk, weight=wk, state=[inp])  # do : b,t,c\n",
    "    dinv, dwv = matmul_backward(dout=dv, weight=wv, state=[inp])  # do : b,t,c\n",
    "    for tnsr in [dwq, dwk, dwv, dinq, dink, dinv]:\n",
    "        assert not tnsr.requires_grad\n",
    "    return dinq + dink + dinv, dwq, dwk, dwv, dwo\n",
    "\n",
    "\n",
    "def init_attention(cnfg: LlamaConfig, cattn: train_llama3.CausalSelfAttention):\n",
    "    # need to init wq, wk, wv, wo from cattn weights\n",
    "    B, C, T, nh, n_kv_head, maxT = (\n",
    "        cnfg.B,\n",
    "        cnfg.n_embd,\n",
    "        cnfg.T,\n",
    "        cnfg.n_head,\n",
    "        cnfg.n_kv_head,\n",
    "        cnfg.block_size,\n",
    "    )\n",
    "    hs = C // nh\n",
    "    kv_dim = hs * n_kv_head\n",
    "    n_rep = nh // n_kv_head\n",
    "    weight = cattn.c_attn.weight.detach().clone()  # has only the weights, no grads\n",
    "    awq, awk, awv = torch.split(weight, [C, kv_dim, kv_dim], dim=0)\n",
    "    awo = cattn.c_proj.weight.detach().clone()\n",
    "    assert awo._cdata != cattn.c_proj.weight._cdata\n",
    "    for tnsr in [awq, awk, awv, awo]:\n",
    "        assert not tnsr.requires_grad\n",
    "    return awq, awk, awv, awo\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    for cnfg in configs:\n",
    "        print(\"Checking: \", cnfg)\n",
    "\n",
    "        B, C, T, nh, n_kv_head, maxT = (\n",
    "            cnfg.B,\n",
    "            cnfg.n_embd,\n",
    "            cnfg.T,\n",
    "            cnfg.n_head,\n",
    "            cnfg.n_kv_head,\n",
    "            cnfg.block_size,\n",
    "        )\n",
    "        hs = C // nh\n",
    "\n",
    "        cos, sin = compute_freqs_cis(hs, maxT, theta=cnfg.rope_theta)\n",
    "        assert not cos.requires_grad\n",
    "        assert not sin.requires_grad\n",
    "\n",
    "        freqs_cis = train_llama3.precompute_freqs_cis(\n",
    "            dim=hs, end=T, theta=cnfg.rope_theta, use_scaled=False\n",
    "        )\n",
    "        assert not freqs_cis.requires_grad\n",
    "\n",
    "        inp = torch.randn((B, T, C))\n",
    "        inp.requires_grad = True\n",
    "        cattn = train_llama3.CausalSelfAttention(cnfg)\n",
    "        mask = torch.triu(\n",
    "            torch.ones((T, T), device=\"cpu\", dtype=torch.bool),\n",
    "            diagonal=1,\n",
    "        )\n",
    "        cattno1 = cattn.forward(x=inp, freqs_cis=freqs_cis, mask=mask)\n",
    "\n",
    "        ### backward\n",
    "        dout = torch.randn_like(cattno1)\n",
    "        cattno1.backward(dout)\n",
    "        kv_dim = hs * n_kv_head\n",
    "        n_rep = nh // n_kv_head\n",
    "\n",
    "        inp1 = inp.detach()\n",
    "        assert not inp1.requires_grad\n",
    "        awq, awk, awv, awo = init_attention(cnfg, cattn)\n",
    "\n",
    "        state = []\n",
    "        cattno2 = attention(cnfg, inp1, awq.T, awk.T, awv.T, awo.T, cos, sin, state)\n",
    "        assert len(state) == 14\n",
    "        assert cattno1.shape == cattno2.shape\n",
    "        assert torch.allclose(\n",
    "            cattno1, cattno2, atol=1e-5\n",
    "        ), f\"\"\"cattno1: {cattno1.view(-1)[:10]}\n",
    "                                                            cattno2: {cattno2.view(-1)[:10]}\"\"\"\n",
    "        print(\"\\tForward pass matches\")\n",
    "\n",
    "        bawo = cattn.c_proj.weight.grad.T\n",
    "        bawq, bawk, bawv = torch.split(\n",
    "            cattn.c_attn.weight.grad, [C, kv_dim, kv_dim], dim=0\n",
    "        )\n",
    "        bawq, bawk, bawv = bawq.T, bawk.T, bawv.T\n",
    "\n",
    "        dainp, dawq, dawk, dawv, dawo = attention_backward(\n",
    "            dout, awq.T, awk.T, awv.T, awo.T, cos, sin, state\n",
    "        )\n",
    "\n",
    "        assert torch.allclose(dainp, inp.grad, atol=1e-4)\n",
    "        assert torch.allclose(dawq, bawq, atol=1e-4)\n",
    "        assert torch.allclose(dawk, bawk, atol=1e-4)\n",
    "        assert torch.allclose(dawv, bawv, atol=1e-4)\n",
    "        assert torch.allclose(dawo, bawo, atol=1e-4)\n",
    "        print(\"        Backward pass matches\\n\")\n",
    "        print(\"--------\")\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d71022-e479-4e69-8f14-4dd75548188a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "313b8e7f-b2d7-4ef6-9661-36ebc1a8ce16",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def block(\n",
    "    config: LlamaConfig,\n",
    "    inp: torch.Tensor,\n",
    "    attn_norm_w: torch.Tensor,\n",
    "    wq: torch.Tensor,\n",
    "    wk: torch.Tensor,\n",
    "    wv: torch.Tensor,\n",
    "    wo: torch.Tensor,\n",
    "    ff_norm_w: torch.Tensor,\n",
    "    w1: torch.Tensor,\n",
    "    w2: torch.Tensor,\n",
    "    w3: torch.Tensor,\n",
    "    cos_theta,\n",
    "    sin_theta,\n",
    "    state,\n",
    "):\n",
    "    ln1x = rms_forward(config.norm_eps, inp, attn_norm_w, state)\n",
    "    cprint(\"ln_1_: \", ln1x.view(-1)[-15:], debug=DEBUG)\n",
    "    attn = attention(config, ln1x, wq, wk, wv, wo, cos_theta, sin_theta, state)\n",
    "    cprint(\"attn_: \", attn.view(-1)[-15:], debug=DEBUG)\n",
    "    h = inp + attn\n",
    "    ln2x = rms_forward(config.norm_eps, h, ff_norm_w, state)\n",
    "    cprint(\"ln_2_: \", ln2x.view(-1)[-15:], debug=DEBUG)\n",
    "    o1 = ff(ln2x, w1, w2, w3, state)\n",
    "    cprint(\"mlp_: \", o1.view(-1)[-15:], debug=DEBUG)\n",
    "    h = h + o1\n",
    "    cprint(\"f_res_: \", h.view(-1)[-15:], debug=DEBUG)\n",
    "    return h\n",
    "\n",
    "\n",
    "def block_backward(\n",
    "    config: LlamaConfig,\n",
    "    dout: torch.Tensor,\n",
    "    attn_norm_w: torch.Tensor,\n",
    "    wq: torch.Tensor,\n",
    "    wk: torch.Tensor,\n",
    "    wv: torch.Tensor,\n",
    "    wo: torch.Tensor,\n",
    "    ff_norm_w: torch.Tensor,\n",
    "    w1: torch.Tensor,\n",
    "    w2: torch.Tensor,\n",
    "    w3: torch.Tensor,\n",
    "    cos_theta,\n",
    "    sin_theta,\n",
    "    state=[],\n",
    "):\n",
    "    (\n",
    "        inp,\n",
    "        attn_norm,\n",
    "        norm_eps,\n",
    "        a_inp,\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        qr,\n",
    "        kr,\n",
    "        k_e,\n",
    "        v_e,\n",
    "        qp,\n",
    "        kT,\n",
    "        qkT,\n",
    "        scores,\n",
    "        o,\n",
    "        vp,\n",
    "        ff_inp,\n",
    "        ff_norm,\n",
    "        _,\n",
    "        mlp_inp,\n",
    "        h1,\n",
    "        h3,\n",
    "        h1silu,\n",
    "        h1h3,\n",
    "    ) = state\n",
    "    dh1, do1 = dout, dout\n",
    "    d_mlp_inp, dw1, dw2, dw3 = ff_backward(\n",
    "        do1, w1, w2, w3, state=[mlp_inp, h1, h3, h1silu, h1h3]\n",
    "    )\n",
    "    d_ff_inp, d_ff_norm_w = rms_backward(\n",
    "        d_mlp_inp, ff_norm_w, state=[ff_inp, ff_norm, norm_eps]\n",
    "    )\n",
    "    dh = dh1 + d_ff_inp\n",
    "    dinp1, dattn = dh, dh\n",
    "    d_a_inp, dwq, dwk, dwv, dwo = attention_backward(\n",
    "        dh,\n",
    "        wq,\n",
    "        wk,\n",
    "        wv,\n",
    "        wo,\n",
    "        cos_theta,\n",
    "        sin_theta,\n",
    "        state=[a_inp, q, k, v, qr, kr, k_e, v_e, qp, kT, qkT, scores, o, vp],\n",
    "    )\n",
    "    d_attn_norm_inp, d_attn_norm_w = rms_backward(\n",
    "        d_a_inp, attn_norm_w, state=[inp, attn_norm, norm_eps]\n",
    "    )\n",
    "    return (\n",
    "        dinp1 + d_attn_norm_inp,\n",
    "        d_attn_norm_w,\n",
    "        dwq,\n",
    "        dwk,\n",
    "        dwv,\n",
    "        dwo,\n",
    "        d_ff_norm_w,\n",
    "        dw1,\n",
    "        dw2,\n",
    "        dw3,\n",
    "    )\n",
    "\n",
    "# init custom block from the llama3 block\n",
    "def init_from_block(config: LlamaConfig, block: train_llama3.Block):\n",
    "\n",
    "    B, T, nh, n_kv_head, C = (\n",
    "        config.B,\n",
    "        config.T,\n",
    "        config.n_head,\n",
    "        config.n_kv_head,\n",
    "        config.n_embd,\n",
    "    )\n",
    "    hs = C // nh\n",
    "\n",
    "    # attn norm\n",
    "    ln_1 = block.ln_1.weight.detach().clone()\n",
    "    assert not ln_1.requires_grad\n",
    "    awq, awk, awv, awo = init_attention(config, block.attn)\n",
    "    ln_2 = block.ln_2.weight.detach().clone()\n",
    "    assert not ln_2.requires_grad\n",
    "    w3 = block.mlp.c_fc.weight.detach().clone()\n",
    "    w1 = block.mlp.c_fc2.weight.detach().clone()\n",
    "    w2 = block.mlp.c_proj.weight.detach().clone()\n",
    "    for tnsr in [w1, w2, w3]:\n",
    "        assert not tnsr.requires_grad\n",
    "    return ln_1, awq, awk, awv, awo, ln_2, w1, w2, w3\n",
    "\n",
    "\n",
    "if  TEST:\n",
    "    for cnfg in configs:\n",
    "        print(\"\\nChecking: \", cnfg)\n",
    "        B, T, C, nh, n_kv_head, maxT = (\n",
    "            cnfg.B,\n",
    "            cnfg.T,\n",
    "            cnfg.n_embd,\n",
    "            cnfg.n_head,\n",
    "            cnfg.n_kv_head,\n",
    "            cnfg.block_size,\n",
    "        )\n",
    "        blk = train_llama3.Block(cnfg)\n",
    "        ln_1, awq, awk, awv, awo, ln_2, w1, w2, w3 = init_from_block(cnfg, blk)\n",
    "        # attn norm\n",
    "        attn_norm_w = blk.ln_1.weight\n",
    "\n",
    "        cattn = blk.attn\n",
    "        mask = torch.triu(\n",
    "            torch.ones((T, T), device=\"cpu\", dtype=torch.bool),\n",
    "            diagonal=1,\n",
    "        )\n",
    "        hs = C // nh\n",
    "        cos, sin = compute_freqs_cis(hs, maxT, cnfg.rope_theta)\n",
    "        freqs_cis = train_llama3.precompute_freqs_cis(\n",
    "            dim=hs, end=T, theta=cnfg.rope_theta, use_scaled=False\n",
    "        )\n",
    "\n",
    "        kv_dim = hs * n_kv_head\n",
    "        n_rep = nh // n_kv_head\n",
    "\n",
    "        # ff norm\n",
    "        ff_norm_w = blk.ln_2.weight\n",
    "\n",
    "        # feed forward\n",
    "        ffn = blk.mlp\n",
    "        ffw1 = ffn.c_fc2.weight\n",
    "        ffw2 = ffn.c_proj.weight\n",
    "        ffw3 = ffn.c_fc.weight\n",
    "\n",
    "        # output\n",
    "        inp = torch.randn((B, T, C))\n",
    "        inp.requires_grad = True\n",
    "        blko1 = blk(x=inp, freqs_cis=freqs_cis, mask=mask)\n",
    "        dout = torch.randn_like(inp)\n",
    "        blko1.backward(dout)\n",
    "        inp1 = inp.detach()\n",
    "        inp1.requires_grad = False\n",
    "        state = []\n",
    "        \n",
    "        blko2 = block(\n",
    "            cnfg,\n",
    "            inp1,\n",
    "            ln_1,\n",
    "            awq.T,\n",
    "            awk.T,\n",
    "            awv.T,\n",
    "            awo.T,\n",
    "            ln_2,\n",
    "            w1.T,\n",
    "            w2.T,\n",
    "            w3.T,\n",
    "            cos,\n",
    "            sin,\n",
    "            state,\n",
    "        )\n",
    "        assert torch.allclose(blko1, blko2, atol=1e-5)\n",
    "\n",
    "        # backward\n",
    "        dawq, dawk, dawv = torch.split(\n",
    "            cattn.c_attn.weight.grad, [C, kv_dim, kv_dim], dim=0\n",
    "        )\n",
    "        dawq, dawk, dawv = dawq.T, dawk.T, dawv.T\n",
    "        dawo = cattn.c_proj.weight.grad.T\n",
    "        d_binp, d_attn_norm_w, dwq, dwk, dwv, dwo, d_ff_norm_w, dw1, dw2, dw3 = (\n",
    "            block_backward(\n",
    "                config3,\n",
    "                dout,\n",
    "                ln_1,\n",
    "                awq.T,\n",
    "                awk.T,\n",
    "                awv.T,\n",
    "                awo.T,\n",
    "                ln_2,\n",
    "                w1.T,\n",
    "                w2.T,\n",
    "                w3.T,\n",
    "                cos,\n",
    "                sin,\n",
    "                state,\n",
    "            )\n",
    "        )\n",
    "        assert torch.allclose(d_binp, inp.grad, atol=1e-4)\n",
    "        assert torch.allclose(d_attn_norm_w, attn_norm_w.grad, atol=1e-4)\n",
    "        assert torch.allclose(dwq, dawq, atol=1e-4)\n",
    "        assert torch.allclose(dwk, dawk, atol=1e-4)\n",
    "        assert torch.allclose(dwv, dawv, atol=1e-4)\n",
    "        assert torch.allclose(dwo, dawo, atol=1e-4)\n",
    "        assert torch.allclose(d_ff_norm_w, ff_norm_w.grad, atol=1e-4)\n",
    "        assert torch.allclose(dw1, ffw1.grad.T, atol=1e-4)\n",
    "        assert torch.allclose(dw2, ffw2.grad.T, atol=1e-4)\n",
    "        assert torch.allclose(dw3, ffw3.grad.T, atol=1e-4)\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279f966",
   "metadata": {},
   "source": [
    "## Adam Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0ad8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(\n",
    "    w: torch.tensor,\n",
    "    dw: torch.tensor,\n",
    "    m_dw: torch.tensor,\n",
    "    var_dw: torch.tensor,\n",
    "    learning_rate: float,\n",
    "    beta1: float,\n",
    "    beta2: float,\n",
    "    eps: float,\n",
    "    weight_decay: float,\n",
    "    t: int,\n",
    "):\n",
    "    bias_correction1 = 1 - beta1**t\n",
    "    bias_correction2 = 1 - beta2**t\n",
    "\n",
    "    m_dw.mul_(beta1).add_(1 - beta1, dw)\n",
    "    var_dw.mul_(beta2).addcmul_(1 - beta2, dw, dw)\n",
    "    denom = (var_dw.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "    step_size = learning_rate / bias_correction1\n",
    "    w.addcdiv_(-step_size, m_dw, denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd82b9-6951-467d-8f5e-e19b79e62448",
   "metadata": {},
   "source": [
    "## Classes\n",
    "    Wraps the forward, backward functions into a single class\n",
    "    Also update, to change the param values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a060f93e-02da-4db9-9b38-22f3bd188fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, V: int, C: int, wte: torch.Tensor = None):\n",
    "        self.wte = wte if wte is not None else torch.zeros(V, C)\n",
    "        self.dwte = None\n",
    "        self.m_dw, self.v_dw = None, None\n",
    "        self.state = []\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return encoder_forward(self.wte, tokens, self.state)\n",
    "\n",
    "    def backward(self, dout: torch.Tensor):\n",
    "        self.dwte = encoder_back(dout=dout, state=self.state)\n",
    "        self.state = []   # reset for the next forward pass\n",
    "\n",
    "        return self.dwte\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        beta1: float,\n",
    "        beta2: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        iter: int,\n",
    "    ):\n",
    "        if self.v_dw is None or self.m_dw is None:\n",
    "            self.m_dw = torch.zeros_like(self.dwte)\n",
    "            self.v_dw = torch.zeros_like(self.dwte)\n",
    "        adam_update(\n",
    "            self.wte,\n",
    "            self.dwte,\n",
    "            self.m_dw,\n",
    "            self.v_dw,\n",
    "            learning_rate,\n",
    "            beta1,\n",
    "            beta2,\n",
    "            eps,\n",
    "            weight_decay,\n",
    "            iter,\n",
    "        )\n",
    "        self.dwte = None\n",
    "\n",
    "\n",
    "class RMS:\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.weight = torch.ones(config.n_embd)\n",
    "        self.dweight = None\n",
    "        self.state = []\n",
    "        self.m_dw, self.v_dw = None, None\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        return rms_forward(self.config.norm_eps, inp, self.weight, self.state)\n",
    "\n",
    "    def backward(self, dout: torch.Tensor):\n",
    "        dinp, self.dweight = rms_backward(dout, self.weight, self.state)\n",
    "        self.state = []\n",
    "        return dinp\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        beta1: float,\n",
    "        beta2: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        iter: int,\n",
    "    ):\n",
    "        if self.m_dw is None or self.v_dw is None:\n",
    "            self.m_dw = torch.zeros_like(self.weight)\n",
    "            self.v_dw = torch.zeros_like(self.weight)\n",
    "        adam_update(\n",
    "            self.weight,\n",
    "            self.dweight,\n",
    "            self.m_dw,\n",
    "            self.v_dw,\n",
    "            learning_rate,\n",
    "            beta1,\n",
    "            beta2,\n",
    "            eps,\n",
    "            weight_decay,\n",
    "            iter,\n",
    "        )\n",
    "        self.dweight = None\n",
    "\n",
    "\n",
    "class Matmul:\n",
    "    def __init__(self, isize: int, osize: int):\n",
    "        super().__init__()\n",
    "        self.weight = torch.zeros(isize, osize)\n",
    "        self.state = []\n",
    "        self.dweight = None\n",
    "        self.m_dw, self.v_dw = None, None\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        return matmul(inp, self.weight, self.state)\n",
    "\n",
    "    def backward(self, dout: torch.Tensor):\n",
    "        dinp, self.dweight = matmul_backward(dout, self.weight, self.state)\n",
    "        self.state = []\n",
    "        return dinp\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        beta1: float,\n",
    "        beta2: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        iter: int,\n",
    "    ):\n",
    "        if self.m_dw is None or self.v_dw is None:\n",
    "            self.m_dw = torch.zeros_like(self.weight)\n",
    "            self.v_dw = torch.zeros_like(self.weight)\n",
    "        adam_update(\n",
    "            self.weight,\n",
    "            self.dweight,\n",
    "            self.m_dw,\n",
    "            self.v_dw,\n",
    "            learning_rate,\n",
    "            beta1,\n",
    "            beta2,\n",
    "            eps,\n",
    "            weight_decay,\n",
    "            iter,\n",
    "        )\n",
    "        self.dweight = None\n",
    "\n",
    "\n",
    "class Silu:\n",
    "    def __init__(self):\n",
    "        self.state = []\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        return silu(inp, self.state)\n",
    "\n",
    "    def backward(self, dout: torch.Tensor):\n",
    "        dinp = silu_backward(dout, self.state)\n",
    "        self.state = []\n",
    "        return dinp\n",
    "\n",
    "\n",
    "class FeedForward:\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        self.state = []\n",
    "        hidden_dim = 4 * config.n_embd\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if config.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(config.ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = config.multiple_of * (\n",
    "            (hidden_dim + config.multiple_of - 1) // config.multiple_of\n",
    "        )\n",
    "        OC = hidden_dim\n",
    "        self.config = config\n",
    "        C, hidden_dim = self.config.n_embd, OC\n",
    "        self.w3 = torch.zeros(C, hidden_dim)  # c_fc\n",
    "        self.w1 = torch.zeros(C, hidden_dim)  # c_fc2\n",
    "        self.w2 = torch.zeros(hidden_dim, C)  # c_proj\n",
    "        (\n",
    "            self.dw1,\n",
    "            self.m_dw1,\n",
    "            self.v_dw1,\n",
    "            self.dw2,\n",
    "            self.m_dw2,\n",
    "            self.v_dw2,\n",
    "            self.dw3,\n",
    "            self.m_dw3,\n",
    "            self.v_dw3,\n",
    "        ) = [None] * 9\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        return ff(inp, self.w1, self.w2, self.w3, self.state)\n",
    "\n",
    "    def backward(self, dout: torch.Tensor):\n",
    "        dinp, self.dw1, self.dw2, self.dw3 = ff_backward(\n",
    "            dout, self.w1, self.w2, self.w3, self.state\n",
    "        )\n",
    "        self.state = []\n",
    "        return dinp\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        beta1: float,\n",
    "        beta2: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        iter: int,\n",
    "    ):\n",
    "        if any(\n",
    "            [\n",
    "                t is None\n",
    "                for t in [\n",
    "                    self.m_dw1,\n",
    "                    self.v_dw1,\n",
    "                    self.m_dw2,\n",
    "                    self.v_dw2,\n",
    "                    self.m_dw3,\n",
    "                    self.v_dw3,\n",
    "                ]\n",
    "            ]\n",
    "        ):\n",
    "            self.m_dw1 = torch.zeros_like(self.dw1)\n",
    "            self.v_dw1 = torch.zeros_like(self.dw1)\n",
    "            self.m_dw2 = torch.zeros_like(self.dw2)\n",
    "            self.v_dw2 = torch.zeros_like(self.dw2)\n",
    "            self.m_dw3 = torch.zeros_like(self.dw3)\n",
    "            self.v_dw3 = torch.zeros_like(self.dw3)\n",
    "        for w, dw, m_dw, v_dw in [\n",
    "            (self.w1, self.dw1, self.m_dw1, self.v_dw1),\n",
    "            (self.w2, self.dw2, self.m_dw2, self.v_dw2),\n",
    "            (self.w3, self.dw3, self.m_dw3, self.v_dw3),\n",
    "        ]:\n",
    "\n",
    "            adam_update(\n",
    "                w,\n",
    "                dw,\n",
    "                m_dw,\n",
    "                v_dw,\n",
    "                learning_rate,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                eps,\n",
    "                weight_decay,\n",
    "                iter,\n",
    "            )\n",
    "        self.dw1 = None\n",
    "        self.dw2 = None\n",
    "        self.dw3 = None\n",
    "\n",
    "\n",
    "class Repeat:\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        self.state = []\n",
    "        self.n_rep = config.n_rep\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        return repeat_kv(inp, self.n_rep, self.state)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dinp = repeat_kv_backward(dout, self.state)\n",
    "        self.state = []\n",
    "        return dinp\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    \n",
    "    def __init__(self, config, cos, sin):\n",
    "        C, nh, n_kv_head = config.n_embd, config.n_head, config.n_kv_head\n",
    "        kv_dim = (C // nh) * n_kv_head\n",
    "        self.wq = torch.zeros(C, C)\n",
    "        self.wk = torch.zeros(C, kv_dim)\n",
    "        self.wv = torch.zeros(C, kv_dim)\n",
    "        self.wo = torch.zeros(C, C)\n",
    "        self.config = config\n",
    "        self.state = []\n",
    "        self.cos, self.sin = cos, sin\n",
    "\n",
    "        (\n",
    "            self.dwq,\n",
    "            self.m_dwq,\n",
    "            self.v_dwq,\n",
    "            self.dwk,\n",
    "            self.m_dwk,\n",
    "            self.v_dwk,\n",
    "            self.dwv,\n",
    "            self.m_dwv,\n",
    "            self.v_dwv,\n",
    "            self.dwo,\n",
    "            self.m_dwo,\n",
    "            self.v_dwo,\n",
    "        ) = [None] * 12\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        return attention(\n",
    "            self.config,\n",
    "            inp,\n",
    "            self.wq,\n",
    "            self.wk,\n",
    "            self.wv,\n",
    "            self.wo,\n",
    "            self.cos,\n",
    "            self.sin,\n",
    "            self.state,\n",
    "        )\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dinp, self.dwq, self.dwk, self.dwv, self.dwo = attention_backward(\n",
    "            dout, self.wq, self.wk, self.wv, self.wo, self.cos, self.sin, self.state\n",
    "        )\n",
    "        self.state = []\n",
    "        return dinp\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        beta1: float,\n",
    "        beta2: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        iter: int,\n",
    "    ):\n",
    "        if any(\n",
    "            [\n",
    "                t is None\n",
    "                for t in [\n",
    "                    self.m_dwq,\n",
    "                    self.v_dwq,\n",
    "                    self.m_dwk,\n",
    "                    self.v_dwk,\n",
    "                    self.m_dwv,\n",
    "                    self.v_dwv,\n",
    "                    self.m_dwo,\n",
    "                    self.v_dwo,\n",
    "                ]\n",
    "            ]\n",
    "        ):\n",
    "            self.m_dwq = torch.zeros_like(self.dwq)\n",
    "            self.v_dwq = torch.zeros_like(self.dwq)\n",
    "            self.m_dwk = torch.zeros_like(self.dwk)\n",
    "            self.v_dwk = torch.zeros_like(self.dwk)\n",
    "            self.m_dwv = torch.zeros_like(self.dwv)\n",
    "            self.v_dwv = torch.zeros_like(self.dwv)\n",
    "            self.m_dwo = torch.zeros_like(self.dwo)\n",
    "            self.v_dwo = torch.zeros_like(self.dwo)\n",
    "        for w, dw, m_dw, v_dw in [\n",
    "            (self.wq, self.dwq, self.m_dwq, self.v_dwq),\n",
    "            (self.wk, self.dwk, self.m_dwk, self.v_dwk),\n",
    "            (self.wv, self.dwv, self.m_dwv, self.v_dwv),\n",
    "            (self.wo, self.dwo, self.m_dwo, self.v_dwo),\n",
    "        ]:\n",
    "            adam_update(\n",
    "                w,\n",
    "                dw,\n",
    "                m_dw,\n",
    "                v_dw,\n",
    "                learning_rate,\n",
    "                beta1,\n",
    "                beta2,\n",
    "                eps,\n",
    "                weight_decay,\n",
    "                iter,\n",
    "            )\n",
    "        self.dwq = None\n",
    "        self.dwk = None\n",
    "        self.dwv = None\n",
    "        self.dwo = None\n",
    "\n",
    "\n",
    "class MyBlock:\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, cos, sin):\n",
    "        self.ln_1 = RMS(config)\n",
    "        self.attention = Attention(config, cos, sin)\n",
    "        self.ln_2 = RMS(config)\n",
    "        self.ff = FeedForward(config)\n",
    "        self.config = config\n",
    "        self.state = []\n",
    "\n",
    "    def forward_(self, inp: torch.Tensor):\n",
    "        h = inp + self.attention.forward(self.ln_1.forward(inp))\n",
    "        return h + self.ff.forward(self.ln_2.forward(h))\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        ln1x = self.ln_1.forward(inp)\n",
    "        self.state.extend(self.ln_1.state)\n",
    "        cprint(\"ln_1_: \", ln1x.view(-1)[-15:], debug=DEBUG)\n",
    "        attn = self.attention.forward(ln1x)\n",
    "        self.state.extend(self.attention.state)\n",
    "        cprint(\"attn_: \", attn.view(-1)[-15:], debug=DEBUG)\n",
    "        h = inp + attn\n",
    "        ln2x = self.ln_2.forward(h)\n",
    "        self.state.extend(self.ln_2.state)\n",
    "        cprint(\"ln_2_: \", ln2x.view(-1)[-15:], debug=DEBUG)\n",
    "        mlp = self.ff.forward(ln2x)\n",
    "        self.state.extend(self.ff.state)\n",
    "        h = h + mlp\n",
    "        cprint(\"f_res_: \", h.view(-1)[-15:], debug=DEBUG)\n",
    "        return h\n",
    "\n",
    "    def backward(self, dout):\n",
    "\n",
    "        dh1, do1 = dout, dout\n",
    "        d_mlp_inp = self.ff.backward(do1)\n",
    "        d_ff_inp = self.ln_2.backward(d_mlp_inp)\n",
    "        dh = dh1 + d_ff_inp\n",
    "        dinp1, dattn = dh, dh\n",
    "        d_a_inp = self.attention.backward(dh)\n",
    "        d_attn_norm_inp = self.ln_1.backward(d_a_inp)\n",
    "        return dinp1 + d_attn_norm_inp\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        beta1: float,\n",
    "        beta2: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        iter: int,\n",
    "    ):\n",
    "        self.ln_1.update(learning_rate, beta1, beta2, eps, weight_decay, iter)\n",
    "        self.attention.update(learning_rate, beta1, beta2, eps, weight_decay, iter)\n",
    "        self.ln_2.update(learning_rate, beta1, beta2, eps, weight_decay, iter)\n",
    "        self.ff.update(learning_rate, beta1, beta2, eps, weight_decay, iter)\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(V=config.vocab_size, C=config.n_embd)\n",
    "        self.output = Matmul(isize=config.n_embd, osize=config.vocab_size)\n",
    "        self.loss_state = []\n",
    "\n",
    "        hs = config.n_embd // config.n_head\n",
    "        self.cos, self.sin = compute_freqs_cis(hs, config.block_size, config.rope_theta)\n",
    "        self.blocks = [\n",
    "            MyBlock(config, self.cos, self.sin) for l in range(config.n_layer)\n",
    "        ]\n",
    "        self.ln_f = RMS(config)\n",
    "\n",
    "    def forward(self, inp: torch.Tensor, targets: torch.Tensor = None):\n",
    "        B, T = targets.shape\n",
    "        h = self.encoder.forward(inp)\n",
    "        cprint(f\"encoder_: \", h.view(-1)[-15:], debug=DEBUG)\n",
    "\n",
    "        for l in range(self.config.n_layer):\n",
    "            cprint(l, debug=DEBUG)\n",
    "            h = self.blocks[l].forward(h)\n",
    "\n",
    "        h = self.ln_f.forward(h)\n",
    "        cprint(\"ln_f_: \", h.view(-1)[-15:], debug=DEBUG)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            h = self.output.forward(h)  # B,T,V\n",
    "            # sm_h = torch.log(torch.softmax(h, -1))\n",
    "            # loss = -torch.gather(sm_h, -1, targets.view(B, T, 1)).sum() / (B * T)\n",
    "            loss = cross_entropy_forward(h, targets, self.loss_state)\n",
    "            assert len(self.loss_state) == 3\n",
    "        cprint(\"logits_: \", h.view(-1)[-15:], debug=DEBUG)\n",
    "        if targets is None:\n",
    "            h = h[:, [-1], :]\n",
    "\n",
    "        return h, loss\n",
    "\n",
    "    def backward(self, dout: torch.Tensor, is_loss=False):\n",
    "        if not is_loss:\n",
    "            # dout : b,1,v (logits)\n",
    "            dh = self.output.backward(dout)\n",
    "            drms = self.ln_f.backward(dh)\n",
    "            dbo = drms\n",
    "            for i in range(self.config.n_layer - 1, -1, -1):\n",
    "                dbo = self.blocks[i].backward(dbo)\n",
    "\n",
    "            self.encoder.backward(dbo)\n",
    "        else:\n",
    "            # dout is from the scalar loss\n",
    "            dh = cross_entropy_backward(None, self.loss_state)\n",
    "            self.backward(dh, False)\n",
    "        self.loss_state = []\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        beta1: float,\n",
    "        beta2: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        iter: int,\n",
    "    ):\n",
    "        self.encoder.update(learning_rate, beta1, beta2, eps, weight_decay, iter)\n",
    "        self.ln_f.update(learning_rate, beta1, beta2, eps, weight_decay, iter)\n",
    "        self.output.update(learning_rate, beta1, beta2, eps, weight_decay, iter)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            block.update(learning_rate, beta1, beta2, eps, weight_decay, iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deabca5",
   "metadata": {},
   "source": [
    "## Utils\n",
    "    -- Help to load llama3 components (in train_llama3.py) into our custom component\n",
    "    -- Help with testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2203819f-67e2-482f-88eb-43f365c18604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_myblock_from_block(\n",
    "    block: train_llama3.Block, config: LlamaConfig, assert_shape=True\n",
    "):\n",
    "\n",
    "    B, T, nh, n_kv_head, C = (\n",
    "        config.B,\n",
    "        config.T,\n",
    "        config.n_head,\n",
    "        config.n_kv_head,\n",
    "        config.n_embd,\n",
    "    )\n",
    "    hs = C // nh\n",
    "    cos, sin = compute_freqs_cis(hs, config.block_size, config.rope_theta)\n",
    "    myBlock = MyBlock(config, cos=cos, sin=sin)\n",
    "\n",
    "    ln_1, awq, awk, awv, awo, ln_2, w1, w2, w3 = init_from_block(config, block) # clone the params in llama3 block\n",
    "    for tnsr in [ln_1, awq, awk, awv, awo, ln_2, w1, w2, w3]:\n",
    "        assert not tnsr.requires_grad\n",
    "\n",
    "    # attn norm\n",
    "    if assert_shape:\n",
    "        assert myBlock.ln_1.weight.shape == block.ln_1.weight.shape\n",
    "    myBlock.ln_1.weight[...] = ln_1  # block.ln_1.weight.detach()\n",
    "    assert not myBlock.ln_1.weight.requires_grad\n",
    "\n",
    "    # attn\n",
    "    cattn = block.attn\n",
    "    mask = torch.triu(\n",
    "        torch.ones((T, T), device=\"cpu\", dtype=torch.bool),\n",
    "        diagonal=1,\n",
    "    )\n",
    "    hs = C // nh\n",
    "    kv_dim = hs * n_kv_head\n",
    "    n_rep = nh // n_kv_head\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.attention.wq.shape == awq.T.shape\n",
    "    myBlock.attention.wq = awq.T  # awq.detach().T\n",
    "\n",
    "    assert not myBlock.attention.wq.requires_grad\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.attention.wk.shape == awk.T.shape\n",
    "    myBlock.attention.wk = awk.T  # awk.detach().T\n",
    "    assert not myBlock.attention.wk.requires_grad\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.attention.wv.shape == awv.T.shape\n",
    "    myBlock.attention.wv = awv.T  # awv.detach().T\n",
    "    assert not myBlock.attention.wv.requires_grad\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.attention.wo.shape == awo.T.shape\n",
    "    myBlock.attention.wo = awo.T  # awo.detach().T\n",
    "    assert not myBlock.attention.wo.requires_grad\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.ln_2.weight.shape == block.ln_2.weight.shape\n",
    "    oldptr = myBlock.ln_2.weight._cdata\n",
    "    myBlock.ln_2.weight = ln_2  # block.ln_2.weight.detach()\n",
    "    assert not myBlock.ln_2.weight.requires_grad\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.ff.w3.shape == block.mlp.c_fc.weight.transpose(0, 1).shape, (\n",
    "            myBlock.ff.w3.shape,\n",
    "            block.mlp.c_fc.weight.transpose(0, 1).shape,\n",
    "        )\n",
    "    myBlock.ff.w3 = w3.T  # block.mlp.c_fc.weight.detach().transpose(0,1)\n",
    "    assert not myBlock.ff.w3.requires_grad\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.ff.w1.shape == block.mlp.c_fc2.weight.transpose(0, 1).shape\n",
    "    myBlock.ff.w1 = w1.T  # block.mlp.c_fc2.weight.detach().transpose(0,1)\n",
    "    assert not myBlock.ff.w1.requires_grad\n",
    "\n",
    "    if assert_shape:\n",
    "        assert myBlock.ff.w2.shape == block.mlp.c_proj.weight.transpose(0, 1).shape\n",
    "    myBlock.ff.w2 = w2.T  # block.mlp.c_proj.weight.detach().transpose(0,1)\n",
    "\n",
    "    assert not myBlock.ff.w2.requires_grad\n",
    "    return myBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa396f06",
   "metadata": {},
   "source": [
    "### test_block_forward, test_block_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6f687181-e5fd-4324-a017-77382191ce9c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "inp grads also match\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "inp grads also match\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "inp grads also match\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n",
      "inp grads also match\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "inp grads also match\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "def test_block_forward(blk: train_llama3.Block, myblk: MyBlock, bdinp=None, inp=None):\n",
    "    # ff\n",
    "    assert torch.allclose(myblk.ff.w1, blk.mlp.c_fc2.weight.T, atol=1e-4), (\n",
    "        myblk.ff.w1.reshape(-1)[:10],\n",
    "        blk.mlp.c_fc2.weight.grad.reshape(-1)[:10],\n",
    "    )\n",
    "    assert torch.allclose(myblk.ff.w2, blk.mlp.c_proj.weight.T, atol=1e-4), (\n",
    "        myblk.ff.w2.reshape(-1)[:10],\n",
    "        blk.mlp.c_proj.weight.T.reshape(-1)[:10],\n",
    "    )\n",
    "    assert torch.allclose(myblk.ff.w3, blk.mlp.c_fc.weight.T, atol=1e-4), (\n",
    "        myblk.ff.w3.reshape(-1)[:10],\n",
    "        blk.mlp.c_fc.weight.T.reshape(-1)[:10],\n",
    "    )\n",
    "\n",
    "    ## ff norm\n",
    "    assert torch.allclose(myblk.ln_2.weight, blk.ln_2.weight, atol=1e-4), (\n",
    "        myblk.ln_2.weight.reshape(-1)[:10],\n",
    "        blk.ln_2.weight.reshape(-1)[:10],\n",
    "    )\n",
    "\n",
    "    # attention\n",
    "    wq, wk, wv, wo = (\n",
    "        myblk.attention.wq.T,\n",
    "        myblk.attention.wk.T,\n",
    "        myblk.attention.wv.T,\n",
    "        myblk.attention.wo.T,\n",
    "    )\n",
    "    attention_weight = torch.concat([wq, wk, wv], dim=0)\n",
    "    # assert attention_weight.shape == (C+2*kv_dim, C)\n",
    "    assert torch.allclose(attention_weight, blk.attn.c_attn.weight, atol=1e-4), (\n",
    "        attention_weight.reshape(-1)[:10],\n",
    "        blk.attn.c_attn.weight.reshape(-1)[:10],\n",
    "    )\n",
    "    assert torch.allclose(wo, blk.attn.c_proj.weight, atol=1e-4), (\n",
    "        wo.reshape(-1)[:10],\n",
    "        blk.attn.c_proj.weight.reshape(-1)[:10],\n",
    "    )\n",
    "\n",
    "    # attn norm\n",
    "    assert torch.allclose(myblk.ln_1.weight, blk.ln_1.weight, atol=1e-4), (\n",
    "        myblk.ln_1.weight.view(-1)[:10],\n",
    "        blk.ln_1.weight.view(-1)[:10],\n",
    "    )\n",
    "\n",
    "\n",
    "def test_block_backwards(blk, myblk, bdinp=None, inp=None):\n",
    "    # ff\n",
    "    assert torch.allclose(myblk.ff.dw1, blk.mlp.c_fc2.weight.grad.T, atol=1e-4), (\n",
    "        myblk.ff.dw1.view(-1)[:10],\n",
    "        blk.mlp.c_fc2.weight.grad.T.reshape(-1)[:10],\n",
    "    )\n",
    "    assert torch.allclose(myblk.ff.dw2, blk.mlp.c_proj.weight.grad.T, atol=1e-4), (\n",
    "        myblk.ff.dw2.view(-1)[:10],\n",
    "        blk.mlp.c_proj.weight.grad.T.reshape(-1)[:10],\n",
    "    )\n",
    "    assert torch.allclose(myblk.ff.dw3, blk.mlp.c_fc.weight.grad.T, atol=1e-4), (\n",
    "        myblk.ff.dw3.view(-1)[:10],\n",
    "        blk.mlp.c_fc.weight.grad.T.reshape(-1)[:10],\n",
    "    )\n",
    "\n",
    "    ## ff norm\n",
    "    assert torch.allclose(myblk.ln_2.dweight, blk.ln_2.weight.grad, atol=1e-4), (\n",
    "        myblk.ln_2.dweight.view(-1)[:10],\n",
    "        blk.ln_2.weight.grad.view(-1)[:10],\n",
    "    )\n",
    "\n",
    "    # attention\n",
    "    dwq, dwk, dwv, dwo = (\n",
    "        myblk.attention.dwq.T,\n",
    "        myblk.attention.dwk.T,\n",
    "        myblk.attention.dwv.T,\n",
    "        myblk.attention.dwo.T,\n",
    "    )\n",
    "    attention_weight = torch.concat([dwq, dwk, dwv], dim=0)\n",
    "    # assert attention_weight.shape == (C+2*kv_dim, C)\n",
    "    assert torch.allclose(attention_weight, blk.attn.c_attn.weight.grad, atol=1e-4), (\n",
    "        attention_weight.view(-1)[:10],\n",
    "        blk.attn.c_attn.weight.grad.reshape(-1)[:10],\n",
    "    )\n",
    "    assert torch.allclose(dwo, blk.attn.c_proj.weight.grad, atol=1e-4), (\n",
    "        dwo.reshape(-1)[:10],\n",
    "        blk.attn.c_proj.weight.grad.reshape(-1)[:10],\n",
    "    )\n",
    "\n",
    "    # attn norm\n",
    "    assert torch.allclose(myblk.ln_1.dweight, blk.ln_1.weight.grad, atol=1e-4), (\n",
    "        myblk.ln_1.dweight.view(-1)[:10],\n",
    "        blk.ln_1.weight.grad.view(-1)[:10],\n",
    "    )\n",
    "\n",
    "    # inp\n",
    "    if bdinp is not None and inp.grad is not None:\n",
    "        assert torch.allclose(bdinp, inp.grad, atol=1e-4)\n",
    "        print(\"inp grads also match\")\n",
    "\n",
    "\n",
    "if TEST:\n",
    "    for cnfg in configs:\n",
    "        print(\n",
    "            \"\\nChecking: \",\n",
    "            cnfg,\n",
    "        )\n",
    "        inp = torch.randn((cnfg.B, cnfg.T, cnfg.n_embd))\n",
    "        mask = torch.triu(\n",
    "            torch.ones((cnfg.T, cnfg.T), device=\"cpu\", dtype=torch.bool),\n",
    "            diagonal=1,\n",
    "        )\n",
    "        inp.requires_grad = True\n",
    "        hs = cnfg.n_embd // cnfg.n_head\n",
    "        freqs_cis = train_llama3.precompute_freqs_cis(\n",
    "            dim=hs, end=cnfg.T, theta=cnfg.rope_theta, use_scaled=False\n",
    "        )\n",
    "\n",
    "        blk = train_llama3.Block(cnfg)\n",
    "        blko1 = blk(x=inp, freqs_cis=freqs_cis, mask=mask)\n",
    "        dout = torch.randn_like(blko1)\n",
    "        blko1.backward(dout)\n",
    "        inp1 = inp.detach()\n",
    "        myblk = init_myblock_from_block(blk, cnfg)\n",
    "        assert torch.allclose(myblk.forward(inp1), blko1, atol=1e-6)\n",
    "        for tnsr in myblk.state:\n",
    "            if hasattr(tnsr, \"detach\"):\n",
    "                tnsr = tnsr.detach()\n",
    "        bdinp = myblk.backward(dout)\n",
    "        test_block_backwards(blk, myblk, bdinp, inp)\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44849499",
   "metadata": {},
   "source": [
    "### init_mymodel_from_LLama\n",
    "    -- use the llama3 model to initilise our custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3159315a-2973-4424-90e9-5ae5f9308fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mymodel_from_LLama(config: LlamaConfig, model=None, assert_shape=True):\n",
    "    mymodel = Transformer(config)\n",
    "    if model is None:\n",
    "        model = train_llama3.LLaMA(config)\n",
    "\n",
    "    transformer = model.transformer\n",
    "\n",
    "    # wte\n",
    "    assert mymodel.encoder.wte.shape == transformer[\"wte\"].weight.shape\n",
    "    mymodel.encoder.wte = transformer[\"wte\"].weight.detach().clone()\n",
    "    config.vocab_size, config.n_embd = mymodel.encoder.wte.shape\n",
    "\n",
    "    assert mymodel.output.weight.shape == model.lm_head.weight.T.shape, (\n",
    "        mymodel.output.weight.shape,\n",
    "        model.lm_head.weight.T.shape,\n",
    "    )\n",
    "    mymodel.output.weight = model.lm_head.weight.detach().clone().T  # (C,V)\n",
    "\n",
    "    for l in range(config.n_layer):\n",
    "        mymodel.blocks[l] = init_myblock_from_block(\n",
    "            transformer[\"h\"][l], config, assert_shape\n",
    "        )\n",
    "\n",
    "    mymodel.ln_f.weight = transformer[\"ln_f\"].weight.detach().clone()\n",
    "    return mymodel, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42c6e5",
   "metadata": {},
   "source": [
    "### test Update\n",
    "    - -compare the params after a forward and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5751e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0e575cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass: 0\n",
      "All tests pass: 1\n",
      "All tests pass: 2\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "cnfg = config3\n",
    "cnfg.n_layer = 5\n",
    "betas = (0.9, 0.99)\n",
    "lr = 0.005124\n",
    "\n",
    "# test_block_forward(blk, myblk)\n",
    "torch.random.manual_seed(1)\n",
    "mymodel, model = init_mymodel_from_LLama(cnfg)\n",
    "B, T, C = cnfg.B, cnfg.T, cnfg.n_embd\n",
    "tokens = torch.randint(cnfg.vocab_size, (B, T))\n",
    "targets = tokens.clone()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr, betas, cnfg.norm_eps, 0.0)\n",
    "\n",
    "for n in range(3):\n",
    "    optimiser.zero_grad()\n",
    "    ot1, loss1 = model(tokens, targets, return_logits=True)\n",
    "    dout = torch.randn_like(ot1)\n",
    "    ot1.backward(dout)\n",
    "\n",
    "    tokens1 = tokens.detach()\n",
    "    targets1 = targets.detach()\n",
    "    ot2, loss2 = mymodel.forward(tokens1, targets1)\n",
    "\n",
    "    assert torch.allclose(\n",
    "        mymodel.ln_f.weight, model.transformer[\"ln_f\"].weight, atol=1e-3\n",
    "    ), (\n",
    "        n,\n",
    "        mymodel.ln_f.weight.view(-1)[:10],\n",
    "        model.transformer[\"ln_f\"].weight.view(-1)[:10],\n",
    "    )\n",
    "    assert torch.allclose(mymodel.output.weight, model.lm_head.weight.T, atol=1e-3), (\n",
    "        n,\n",
    "        mymodel.output.weight.view(-1)[:10],\n",
    "        model.lm_head.weight.T.reshape(-1)[:10],\n",
    "    )\n",
    "\n",
    "    for i, blk in enumerate(model.transformer[\"h\"]):\n",
    "        myblk = mymodel.blocks[i]\n",
    "        test_block_forward(blk, myblk)\n",
    "    assert torch.allclose(ot1, ot2, atol=1e-3), (\n",
    "        ot1.view(-1)[:10],\n",
    "        ot2.view(-1)[:10],\n",
    "        (n, ot1.view(-1)[-10:], ot2.view(-1)[-10:]),\n",
    "    )\n",
    "    assert torch.allclose(loss1, loss2, atol=1e-3)\n",
    "    # backward\n",
    "    mymodel.backward(dout, False)\n",
    "    optimiser.step()\n",
    "    mymodel.update(lr, betas[0], betas[1], cnfg.norm_eps, 0.0, n + 1)\n",
    "    print(f\"All tests pass: {n}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402bfd4-ecfd-4cee-b8a9-bfeb22cdd89c",
   "metadata": {},
   "source": [
    "## Classes forward+backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "316ea01f-c03c-452a-94ce-870e3e823ab7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    print(len(configs))\n",
    "    for cnfg in configs:\n",
    "        print(cnfg)\n",
    "        B, T, C, V = cnfg.B, cnfg.T, cnfg.n_embd, cnfg.vocab_size\n",
    "\n",
    "        # encoder\n",
    "        enc = nn.Embedding(V, C)\n",
    "        tokens = torch.randint(V, (B, T))\n",
    "        enco1 = enc(tokens)\n",
    "        dout = torch.randn_like(enco1)\n",
    "        enco1.backward(dout, inputs=[enc.weight])\n",
    "\n",
    "        tokens = tokens.detach()\n",
    "        enc2 = Encoder(V, C, enc.weight.detach())\n",
    "        enco2 = enc2.forward(tokens)\n",
    "        assert enco1.shape == enco2.shape\n",
    "        assert torch.allclose(enco1, enco2)\n",
    "        assert torch.allclose(enc2.state[0], tokens)\n",
    "\n",
    "        dwte = enc2.backward(dout)\n",
    "        assert torch.allclose(dwte, enc.weight.grad)\n",
    "\n",
    "        # output\n",
    "        inp = torch.randn(B, T, C)\n",
    "        o1 = nn.Linear(in_features=C, out_features=V, bias=False)\n",
    "        o2 = Matmul(isize=C, osize=V)\n",
    "\n",
    "        o2.weight[...] = o1.weight.detach().T\n",
    "        o1o = o1(inp)\n",
    "        dout = torch.randn_like(o1o)\n",
    "        o1o.backward(dout)\n",
    "        inp1 = inp.detach()\n",
    "\n",
    "        o2o = o2.forward(inp1)\n",
    "        dinp = o2.backward(dout)\n",
    "        assert torch.allclose(o1o, o2o, atol=1e-4)\n",
    "        assert torch.allclose(o2.dweight, o1.weight.grad.T, atol=1e-4)\n",
    "\n",
    "        # rms\n",
    "        inp = torch.randn(B, T, C)\n",
    "        inp.requires_grad = True\n",
    "        rms = train_llama3.RMSNorm(C)\n",
    "        o1 = rms(inp)\n",
    "        dout = torch.randn_like(o1)\n",
    "        o1.backward(dout)\n",
    "        inp1 = inp.detach()\n",
    "\n",
    "        ro2 = RMS(cnfg)\n",
    "        ro2.weight = rms.weight.detach()\n",
    "        o2 = ro2.forward(inp1)\n",
    "        assert torch.allclose(o1, o2, atol=1e-4), (o1.view(-1)[-10:], o2.view(-1)[-10:])\n",
    "\n",
    "        # backward\n",
    "        dinp = ro2.backward(dout)\n",
    "        assert torch.allclose(\n",
    "            ro2.dweight, rms.weight.grad, atol=1e-3\n",
    "        ), f\"{dw1[:5]},{rms.weight.grad[:5]}\"\n",
    "        assert torch.allclose(\n",
    "            dinp, inp.grad, atol=1e-3\n",
    "        ), f\"{dinp.view(-1)[:10]}, {inp.grad.view(-1)[:10]}\"\n",
    "print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41e2a3",
   "metadata": {},
   "source": [
    "## Test Model forward backward\n",
    "    -- Test params after forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "021d94bb-b0fc-44a2-80f1-4c37ec112389",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n",
      "\n",
      "Checking:  LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    for cnfg in configs:\n",
    "        print(\"\\nChecking: \", cnfg)\n",
    "\n",
    "        mymodel, model = init_mymodel_from_LLama(cnfg)   # model : llama3, mymodel: custom llama3\n",
    "        B, T, C = cnfg.B, cnfg.T, cnfg.n_embd\n",
    "        tokens = torch.randint(cnfg.vocab_size, (B, T))\n",
    "        targets = tokens.clone()\n",
    "        ot1, loss1 = model(tokens, targets, return_logits=True)\n",
    "        dout = torch.randn_like(ot1)\n",
    "        ot1.backward(dout)\n",
    "\n",
    "        tokens1 = tokens.detach()\n",
    "        targets1 = targets.detach()\n",
    "        ot2, loss2 = mymodel.forward(tokens1, targets1)\n",
    "        assert torch.allclose(ot1, ot2, atol=1e-4), (\n",
    "            ot1.view(-1)[:10],\n",
    "            ot2.view(-1)[:10],\n",
    "            (ot1.view(-1)[-10:], ot2.view(-1)[-10:]),\n",
    "        )\n",
    "        assert torch.allclose(loss1, loss2, atol=1e-4)\n",
    "        # backward\n",
    "        mymodel.backward(dout)\n",
    "\n",
    "        assert torch.allclose(\n",
    "            mymodel.encoder.dwte, model.transformer[\"wte\"].weight.grad, atol=1e-4\n",
    "        ), (\n",
    "            mymodel.encoder.dwte.view(-1)[:10],\n",
    "            model.transformer[\"wte\"].weight.grad.view(-1)[:10],\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            mymodel.ln_f.dweight, model.transformer[\"ln_f\"].weight.grad, atol=1e-4\n",
    "        ), (\n",
    "            mymodel.ln_f.dweight.view(-1)[:10],\n",
    "            model.transformer[\"ln_f\"].weight.grad.view(-1)[:10],\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            mymodel.output.dweight, model.lm_head.weight.grad.T, atol=1e-3\n",
    "        ), (\n",
    "            mymodel.output.dweight.view(-1)[:10],\n",
    "            model.lm_head.weight.grad.T.reshape(-1)[:10],\n",
    "        )\n",
    "        for i, blk in enumerate(model.transformer[\"h\"]):\n",
    "            myblk = mymodel.blocks[i]\n",
    "            test_block_backwards(blk, myblk)\n",
    "    print(\"PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746da09a",
   "metadata": {},
   "source": [
    "## Testing loss for the different configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ae3497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def train_model(\n",
    "    model: Union[train_llama3.LLaMA, Transformer],\n",
    "    inputs: torch.tensor,\n",
    "    targets: torch.tensor,\n",
    "    n_train_steps=10,\n",
    "):\n",
    "    optimiser = None\n",
    "    if isinstance(model, train_llama3.LLaMA):\n",
    "        forward = lambda: model.forward(inputs, targets, True, 0)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr, betas, eps, weight_decay)\n",
    "        update = lambda iter: (optimiser.step(), optimiser.zero_grad())\n",
    "\n",
    "    else:\n",
    "        forward = lambda: model.forward(inputs, targets)\n",
    "        update = lambda iter: model.update(\n",
    "            lr, betas[0], betas[1], eps, weight_decay, iter\n",
    "        )\n",
    "\n",
    "    logits_loss = []\n",
    "    for n in range(n_train_steps):\n",
    "        if isinstance(model, train_llama3.LLaMA):\n",
    "            optimiser.zero_grad()\n",
    "        lg, loss = forward()\n",
    "        if isinstance(model, train_llama3.LLaMA):\n",
    "            loss.backward()  # get the grad\n",
    "\n",
    "        else:\n",
    "            model.backward(None, True)\n",
    "        update(n + 1)\n",
    "        logits_loss.append((lg, loss))\n",
    "    return logits_loss\n",
    "\n",
    "\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_models(\n",
    "    llama_model: train_llama3.LLaMA,\n",
    "    mymodel: Transformer,\n",
    "    inputs: torch.tensor,\n",
    "    targets: torch.tensor,\n",
    "    n_train_steps=10,\n",
    "    params={},\n",
    "):\n",
    "    eps = params[\"eps\"]\n",
    "    lr = params[\"lr\"]\n",
    "    betas = params[\"betas\"]\n",
    "    weight_decay = 0.0\n",
    "    optimiser = torch.optim.Adam(llama_model.parameters(), lr, betas, eps, weight_decay)\n",
    "\n",
    "    inputs1 = inputs.detach().clone()\n",
    "    targets1 = targets.detach().clone()\n",
    "    logits_loss = defaultdict(list)\n",
    "    for n in tqdm(range(n_train_steps), desc=\"Training\"):\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward + backward\n",
    "        lg1, loss1 = mymodel.forward(inputs1, targets1)\n",
    "        lg, loss = llama_model.forward(inputs, targets, True)\n",
    "\n",
    "        mymodel.backward(None, True)  # get the grad\n",
    "        loss.backward()  # get the grad\n",
    "\n",
    "        logits_loss[\"llama\"].append((lg, loss))\n",
    "        logits_loss[\"myllama\"].append((lg1, loss1))\n",
    "\n",
    "        optimiser.step()\n",
    "        mymodel.update(lr, betas[0], betas[1], eps, weight_decay, n + 1)\n",
    "        cprint(f\"Done trainig step: {n}\", debug=DEBUG)\n",
    "\n",
    "    return logits_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f9501723",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = LlamaConfig(\n",
    "    version=\"3.1\",\n",
    "    block_size=200,\n",
    "    vocab_size=100,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    n_kv_head=4,\n",
    "    n_embd=48,\n",
    "    ffn_dim_multiplier=1.3,\n",
    "    multiple_of=32,\n",
    "    norm_eps=1e-05,\n",
    "    rope_theta=500000.0,\n",
    "    use_scaled_rope=False,\n",
    "    max_gen_batch_size=4,\n",
    "    use_kv=True,\n",
    "    flash=False,\n",
    "    T=40,\n",
    "    B=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5da6b8-2dc7-497a-9eb1-321ea2986cb1",
   "metadata": {},
   "source": [
    "### Train models (custom llama3, original llama3) and compare the losses, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9409c0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig(version='3.1', block_size=200, vocab_size=100, n_layer=12, n_head=12, n_kv_head=4, n_embd=48, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=40, B=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████| 50/50 [01:38<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel_loss:\n",
      " tensor([[4.7532368, 4.5702333, 4.4022155, 4.2488265, 4.1059370, 3.9710541,\n",
      "         3.8442869, 3.7255216, 3.6128504, 3.5043333, 3.3995423, 3.2988062,\n",
      "         3.2018702, 3.1077132, 3.0154085, 2.9245405, 2.8347578, 2.7454476,\n",
      "         2.6561217, 2.5668652, 2.4777164, 2.3878181, 2.2968245, 2.2051294,\n",
      "         2.1127300, 2.0204105, 1.9284750, 1.8373041, 1.7465363, 1.6566626,\n",
      "         1.5686325, 1.4831506, 1.3998740, 1.3189836, 1.2409533, 1.1672759,\n",
      "         1.0989659, 1.0342294, 0.9672856, 0.9068358, 0.8533952, 0.7996473,\n",
      "         0.7520568, 0.7071263, 0.6667799, 0.6322387, 0.5955911, 0.5605782,\n",
      "         0.5329599, 0.5026795]])\n",
      "ref_model_loss:\n",
      " tensor([4.7532368, 4.5702333, 4.4022155, 4.2488270, 4.1059370, 3.9710541,\n",
      "        3.8442862, 3.7255208, 3.6128504, 3.5043328, 3.3995419, 3.2988060,\n",
      "        3.2018702, 3.1077132, 3.0154085, 2.9245405, 2.8347580, 2.7454481,\n",
      "        2.6561215, 2.5668652, 2.4777167, 2.3878183, 2.2968245, 2.2051296,\n",
      "        2.1127303, 2.0204108, 1.9284754, 1.8373041, 1.7465366, 1.6566625,\n",
      "        1.5686326, 1.4831508, 1.3998741, 1.3189838, 1.2409536, 1.1672759,\n",
      "        1.0989659, 1.0342292, 0.9672855, 0.9068356, 0.8533949, 0.7996472,\n",
      "        0.7520566, 0.7071263, 0.6667799, 0.6322392, 0.5955908, 0.5605786,\n",
      "        0.5329599, 0.5026796])\n",
      "Training results match: LlamaConfig(version='3.1', block_size=200, vocab_size=100, n_layer=12, n_head=12, n_kv_head=4, n_embd=48, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=40, B=10)\n",
      "{'lr': 0.001, 'betas': (0.9, 0.99), 'eps': 1e-08}\n",
      "------------\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=0, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████| 50/50 [01:16<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel_loss:\n",
      " tensor([[3.1669021, 3.0779817, 2.9925590, 2.9107370, 2.8326106, 2.7582567,\n",
      "         2.6877360, 2.6210961, 2.5583651, 2.4995492, 2.4446223, 2.3935208,\n",
      "         2.3461347, 2.3023086, 2.2618463, 2.2245216, 2.1900916, 2.1583102,\n",
      "         2.1289387, 2.1017573, 2.0765696, 2.0532064, 2.0315263, 2.0114117,\n",
      "         1.9927657, 1.9755046, 1.9595535, 1.9448404, 1.9312953, 1.9188451,\n",
      "         1.9074154, 1.8969290, 1.8873073, 1.8784715, 1.8703456, 1.8628561,\n",
      "         1.8559358, 1.8495258, 1.8435745, 1.8380411, 1.8328925, 1.8281031,\n",
      "         1.8236512, 1.8195184, 1.8156873, 1.8121387, 1.8088526, 1.8058076,\n",
      "         1.8029820, 1.8003536]])\n",
      "ref_model_loss:\n",
      " tensor([3.1669018, 3.0779815, 2.9925592, 2.9107370, 2.8326106, 2.7582567,\n",
      "        2.6877358, 2.6210961, 2.5583653, 2.4995492, 2.4446223, 2.3935206,\n",
      "        2.3461344, 2.3023083, 2.2618463, 2.2245216, 2.1900918, 2.1583099,\n",
      "        2.1289384, 2.1017575, 2.0765693, 2.0532064, 2.0315263, 2.0114117,\n",
      "        1.9927659, 1.9755046, 1.9595534, 1.9448406, 1.9312950, 1.9188449,\n",
      "        1.9074154, 1.8969288, 1.8873070, 1.8784716, 1.8703455, 1.8628560,\n",
      "        1.8559358, 1.8495256, 1.8435746, 1.8380411, 1.8328924, 1.8281029,\n",
      "        1.8236511, 1.8195184, 1.8156872, 1.8121386, 1.8088524, 1.8058076,\n",
      "        1.8029819, 1.8003536])\n",
      "Training results match: LlamaConfig(version='3.1', block_size=200, vocab_size=100, n_layer=12, n_head=12, n_kv_head=4, n_embd=48, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=40, B=10)\n",
      "{'lr': 0.001, 'betas': (0.9, 0.99), 'eps': 1e-08}\n",
      "------------\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=1, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████| 50/50 [04:16<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel_loss:\n",
      " tensor([[3.2379425e+00, 2.8768456e+00, 2.5534835e+00, 2.2589612e+00,\n",
      "         2.0057635e+00, 1.8013994e+00, 1.6338880e+00, 1.4878640e+00,\n",
      "         1.3612552e+00, 1.2513376e+00, 1.1428984e+00, 1.0254030e+00,\n",
      "         9.0398687e-01, 7.8603733e-01, 6.7419517e-01, 5.7092136e-01,\n",
      "         4.7738323e-01, 3.9174253e-01, 3.1812239e-01, 2.5913003e-01,\n",
      "         2.0987350e-01, 1.6921929e-01, 1.3559230e-01, 1.0618776e-01,\n",
      "         8.0252700e-02, 5.8618888e-02, 4.3007713e-02, 3.1897701e-02,\n",
      "         2.3648763e-02, 1.7863201e-02, 1.4075876e-02, 1.1427311e-02,\n",
      "         9.6427789e-03, 8.3526038e-03, 7.3383767e-03, 6.4737736e-03,\n",
      "         5.7067149e-03, 5.0379494e-03, 4.4730594e-03, 3.9986628e-03,\n",
      "         3.5951775e-03, 3.2496804e-03, 2.9557205e-03, 2.7074069e-03,\n",
      "         2.4965745e-03, 2.3143049e-03, 2.1536427e-03, 2.0107217e-03,\n",
      "         1.8833457e-03, 1.7696681e-03]])\n",
      "ref_model_loss:\n",
      " tensor([3.2379427e+00, 2.8768454e+00, 2.5534840e+00, 2.2589610e+00,\n",
      "        2.0057640e+00, 1.8013991e+00, 1.6338882e+00, 1.4878640e+00,\n",
      "        1.3612550e+00, 1.2513375e+00, 1.1428984e+00, 1.0254031e+00,\n",
      "        9.0398699e-01, 7.8603733e-01, 6.7419511e-01, 5.7092142e-01,\n",
      "        4.7738326e-01, 3.9174256e-01, 3.1812233e-01, 2.5913000e-01,\n",
      "        2.0987341e-01, 1.6921921e-01, 1.3559225e-01, 1.0618774e-01,\n",
      "        8.0252670e-02, 5.8618847e-02, 4.3007668e-02, 3.1897724e-02,\n",
      "        2.3648759e-02, 1.7863201e-02, 1.4075873e-02, 1.1427308e-02,\n",
      "        9.6427957e-03, 8.3526066e-03, 7.3383874e-03, 6.4737783e-03,\n",
      "        5.7067187e-03, 5.0379522e-03, 4.4730552e-03, 3.9986614e-03,\n",
      "        3.5951810e-03, 3.2496769e-03, 2.9557222e-03, 2.7074059e-03,\n",
      "        2.4965687e-03, 2.3143017e-03, 2.1536434e-03, 2.0107194e-03,\n",
      "        1.8833420e-03, 1.7696670e-03])\n",
      "Training results match: LlamaConfig(version='3.1', block_size=200, vocab_size=100, n_layer=12, n_head=12, n_kv_head=4, n_embd=48, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=40, B=10)\n",
      "{'lr': 0.001, 'betas': (0.9, 0.99), 'eps': 1e-08}\n",
      "------------\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=20, n_layer=2, n_head=8, n_kv_head=4, n_embd=256, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████| 50/50 [06:46<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel_loss:\n",
      " tensor([[3.0214624, 2.4867995, 2.0948153, 1.8162000, 1.6008555, 1.4227315,\n",
      "         1.2761170, 1.1323754, 0.9797642, 0.8291832, 0.6863651, 0.5530309,\n",
      "         0.4375713, 0.3398650, 0.2541281, 0.1832843, 0.1310944, 0.0943556,\n",
      "         0.0675600, 0.0487518, 0.0371242, 0.0277191, 0.0233321, 0.0204118,\n",
      "         0.0187703, 0.0171570, 0.0161513, 0.0155197, 0.0149789, 0.0144951,\n",
      "         0.0140305, 0.0136465, 0.0133863, 0.0131110, 0.0129271, 0.0127457,\n",
      "         0.0125814, 0.0124622, 0.0123197, 0.0122344, 0.0121235, 0.0120549,\n",
      "         0.0119738, 0.0119141, 0.0118550, 0.0118024, 0.0117580, 0.0117125,\n",
      "         0.0116777, 0.0116394]])\n",
      "ref_model_loss:\n",
      " tensor([3.0214624, 2.4867995, 2.0948150, 1.8161999, 1.6008555, 1.4227316,\n",
      "        1.2761171, 1.1323754, 0.9797640, 0.8291833, 0.6863649, 0.5530310,\n",
      "        0.4375713, 0.3398650, 0.2541281, 0.1832844, 0.1310944, 0.0943555,\n",
      "        0.0675600, 0.0487519, 0.0371243, 0.0277191, 0.0233322, 0.0204118,\n",
      "        0.0187703, 0.0171570, 0.0161513, 0.0155197, 0.0149789, 0.0144951,\n",
      "        0.0140305, 0.0136465, 0.0133863, 0.0131110, 0.0129271, 0.0127457,\n",
      "        0.0125814, 0.0124622, 0.0123197, 0.0122344, 0.0121235, 0.0120549,\n",
      "        0.0119738, 0.0119141, 0.0118550, 0.0118024, 0.0117580, 0.0117125,\n",
      "        0.0116777, 0.0116394])\n",
      "Training results match: LlamaConfig(version='3.1', block_size=200, vocab_size=100, n_layer=12, n_head=12, n_kv_head=4, n_embd=48, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=40, B=10)\n",
      "{'lr': 0.001, 'betas': (0.9, 0.99), 'eps': 1e-08}\n",
      "------------\n",
      "LlamaConfig(version='3.1', block_size=10, vocab_size=20, n_layer=5, n_head=4, n_kv_head=2, n_embd=8, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=4, B=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████| 50/50 [00:01<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel_loss:\n",
      " tensor([[3.2748358, 3.2359109, 3.1975098, 3.1596470, 3.1223364, 3.0855932,\n",
      "         3.0494266, 3.0138361, 2.9788167, 2.9443641, 2.9104731, 2.8771393,\n",
      "         2.8443587, 2.8121281, 2.7804418, 2.7492936, 2.7186763, 2.6885798,\n",
      "         2.6589913, 2.6298923, 2.6012564, 2.5730450, 2.5452080, 2.5176854,\n",
      "         2.4904077, 2.4633033, 2.4363050, 2.4093571, 2.3824241, 2.3554990,\n",
      "         2.3286085, 2.3018141, 2.2752039, 2.2488830, 2.2229543, 2.1975040,\n",
      "         2.1725874, 2.1482272, 2.1244147, 2.1011233, 2.0783195, 2.0559745,\n",
      "         2.0340688, 2.0125942, 1.9915466, 1.9709182, 1.9506927, 1.9308451,\n",
      "         1.9113485, 1.8921881]])\n",
      "ref_model_loss:\n",
      " tensor([3.2748358, 3.2359109, 3.1975095, 3.1596467, 3.1223364, 3.0855937,\n",
      "        3.0494266, 3.0138359, 2.9788170, 2.9443641, 2.9104733, 2.8771396,\n",
      "        2.8443587, 2.8121281, 2.7804418, 2.7492936, 2.7186763, 2.6885798,\n",
      "        2.6589913, 2.6298923, 2.6012561, 2.5730448, 2.5452080, 2.5176852,\n",
      "        2.4904072, 2.4633031, 2.4363050, 2.4093573, 2.3824241, 2.3554988,\n",
      "        2.3286085, 2.3018141, 2.2752039, 2.2488830, 2.2229543, 2.1975040,\n",
      "        2.1725874, 2.1482270, 2.1244147, 2.1011231, 2.0783195, 2.0559742,\n",
      "        2.0340691, 2.0125945, 1.9915464, 1.9709179, 1.9506927, 1.9308450,\n",
      "        1.9113485, 1.8921881])\n",
      "Training results match: LlamaConfig(version='3.1', block_size=200, vocab_size=100, n_layer=12, n_head=12, n_kv_head=4, n_embd=48, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=40, B=10)\n",
      "{'lr': 0.001, 'betas': (0.9, 0.99), 'eps': 1e-08}\n",
      "------------\n",
      "LlamaConfig(version='3.1', block_size=64, vocab_size=1000, n_layer=2, n_head=8, n_kv_head=4, n_embd=512, ffn_dim_multiplier=1.3, multiple_of=64, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=32, B=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████| 10/10 [05:32<00:00, 33.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel_loss:\n",
      " tensor([[7.1348028, 5.0838103, 3.1337659, 1.4195486, 0.4036632, 0.1165457,\n",
      "         0.0584250, 0.0382141, 0.0291586, 0.0226412]])\n",
      "ref_model_loss:\n",
      " tensor([7.1348033, 5.0838099, 3.1337662, 1.4195483, 0.4036631, 0.1165457,\n",
      "        0.0584250, 0.0382141, 0.0291586, 0.0226412])\n",
      "Training results match: LlamaConfig(version='3.1', block_size=200, vocab_size=100, n_layer=12, n_head=12, n_kv_head=4, n_embd=48, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=40, B=10)\n",
      "{'lr': 0.001, 'betas': (0.9, 0.99), 'eps': 1e-08}\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_steps = 50\n",
    "lr = 0.001\n",
    "betas = (0.9, 0.99)\n",
    "weight_decay = 0.0\n",
    "eps = 1e-8\n",
    "\n",
    "DEBUG=False\n",
    "import gc\n",
    "torch.random.manual_seed(1)\n",
    "for cnfg in [new_config]+configs:\n",
    "    B, T, V = cnfg.B, cnfg.T, cnfg.vocab_size\n",
    "    inputs = torch.randint(0, V, (B, T))\n",
    "    targets = torch.randint(0, V, (B, T))\n",
    "    \n",
    "    inputs1 = inputs.clone()\n",
    "    targets1 = targets.clone()\n",
    "    \n",
    "    my_llm, llm = init_mymodel_from_LLama(cnfg)\n",
    "    print(cnfg)\n",
    "    params = {\"lr\": lr, \"betas\": betas, \"eps\": eps}\n",
    "    if V >= 500:\n",
    "        loss_logits = train_models(llm, my_llm, inputs, targets, 10, params)  # to save time\n",
    "    else:\n",
    "        loss_logits = train_models(llm, my_llm, inputs, targets, train_steps, params)\n",
    "    llama_loss = torch.tensor([ls.item() for _, ls in loss_logits[\"llama\"]])\n",
    "    my_loss = torch.tensor([[ls for _, ls in loss_logits[\"myllama\"]]])\n",
    "    assert torch.allclose(llama_loss, my_loss, atol=1e-4), (\n",
    "        llama_loss,\n",
    "        \"\\n\",\n",
    "        my_loss,\n",
    "        llama_loss.sum(),\n",
    "        my_loss.sum(),\n",
    "    )\n",
    "    print(\"mymodel_loss:\\n\", my_loss)\n",
    "    print(\"ref_model_loss:\\n\", llama_loss)\n",
    "    print(f\"Training results match: {new_config}\")\n",
    "    print(params)\n",
    "    del my_llm\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549fcbc",
   "metadata": {},
   "source": [
    "## Load model and loss\n",
    "    -- load a gpt2 model\n",
    "    -- initialize our custom model from it\n",
    "    -- compare the losses from the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ee5488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        header = np.frombuffer(buffer=f.read(256 * 4), dtype=np.int32)\n",
    "        maxT, V, L, nH, C = header[2:7]\n",
    "        n_kv_head = nH\n",
    "        hs = C // nH\n",
    "        kv_dim = hs * n_kv_head\n",
    "        print(maxT, V, L, nH, C)\n",
    "        wte = torch.from_numpy(np.frombuffer(f.read(V * C * 4), dtype=np.float32)).view(\n",
    "            V, C\n",
    "        )  # use the transpose of this for the output layer (C,V)\n",
    "        wpe = np.frombuffer(f.read(maxT * C * 4))\n",
    "\n",
    "        l1w = torch.from_numpy(np.frombuffer(f.read(L * C * 4), dtype=np.float32)).view(\n",
    "            L, C\n",
    "        )\n",
    "        np.frombuffer(f.read(L * C * 4))  # (L, C)\n",
    "\n",
    "        c_attn_weight = torch.from_numpy(\n",
    "            np.frombuffer(f.read(L * (C + 2 * kv_dim) * C * 4), dtype=np.float32)\n",
    "        ).view(L, C + 2 * kv_dim, C)\n",
    "\n",
    "        # (L, 3C)\n",
    "        _ = torch.from_numpy(\n",
    "            np.frombuffer(f.read(L * (C + 2 * kv_dim) * 4), dtype=np.float32)\n",
    "        ).view(L, C + 2 * kv_dim)\n",
    "\n",
    "        # (L, C, C)\n",
    "        c_attn_proj = torch.from_numpy(\n",
    "            np.frombuffer(f.read(L * (C) * C * 4), dtype=np.float32)\n",
    "        ).view(L, C, C)\n",
    "\n",
    "        # (L, C)\n",
    "        _ = torch.from_numpy(np.frombuffer(f.read(L * (C) * 4), dtype=np.float32)).view(\n",
    "            L, C\n",
    "        )\n",
    "        # (L, C)\n",
    "        l2w = torch.from_numpy(np.frombuffer(f.read(L * C * 4), dtype=np.float32)).view(\n",
    "            L, C\n",
    "        )\n",
    "        # (L, C)\n",
    "        _ = torch.from_numpy(np.frombuffer(f.read(L * C * 4), dtype=np.float32)).view(\n",
    "            L, C\n",
    "        )\n",
    "\n",
    "        # (L, 4C, C)\n",
    "        OC = 4 * C  # w3 and w1 are identical\n",
    "        mlp_w1 = torch.from_numpy(\n",
    "            np.frombuffer(f.read(L * (4 * C) * C * 4), dtype=np.float32)\n",
    "        ).view(L, 4 * C, C)\n",
    "        \n",
    "        # (L, 4C)\n",
    "        torch.from_numpy(np.frombuffer(f.read(L * (4 * C) * 4), dtype=np.float32))\n",
    "        \n",
    "        # (L, C, 4C)\n",
    "        mlp_w2 = torch.from_numpy(\n",
    "            np.frombuffer(f.read(L * (4 * C) * C * 4), dtype=np.float32)\n",
    "        ).view(L, C, 4 * C)\n",
    "        \n",
    "        # (L, C)\n",
    "        torch.from_numpy(np.frombuffer(f.read(L * C * 4), dtype=np.float32)).view(L, C)\n",
    "\n",
    "        # (C)\n",
    "        lnf = torch.from_numpy(np.frombuffer(f.read(C * 4), dtype=np.float32)).view(C)\n",
    "\n",
    "        torch.from_numpy(np.frombuffer(f.read(C * 4), dtype=np.float32)).view(C)\n",
    "\n",
    "        config = LlamaConfig(\n",
    "            block_size=maxT,\n",
    "            vocab_size=V,\n",
    "            n_layer=L,\n",
    "            n_head=nH,\n",
    "            n_kv_head=nH,\n",
    "            n_embd=C,\n",
    "            n_rep=1,\n",
    "        )\n",
    "\n",
    "\n",
    "        # llama3 model\n",
    "        llama = train_llama3.LLaMA(config)\n",
    "        transformer = llama.transformer\n",
    "\n",
    "        assert transformer[\"wte\"].weight.shape == (V, C), ()\n",
    "        del transformer[\"wte\"].weight\n",
    "        transformer[\"wte\"].weight = nn.Parameter(wte)\n",
    "\n",
    "        del transformer[\"ln_f\"].weight\n",
    "        transformer[\"ln_f\"].weight = nn.Parameter(lnf)\n",
    "\n",
    "        assert llama.lm_head.weight.shape == (V, C)\n",
    "        del llama.lm_head.weight\n",
    "        llama.lm_head.weight = nn.Parameter(wte.detach())\n",
    "\n",
    "        for l in range(L):\n",
    "            assert transformer[\"h\"][l].ln_1.weight.shape == l1w[l].shape\n",
    "            del transformer[\"h\"][l].ln_1.weight\n",
    "            transformer[\"h\"][l].ln_1.weight = nn.Parameter(l1w[l])\n",
    "\n",
    "            assert transformer[\"h\"][l].ln_2.weight.shape == l2w[l].shape\n",
    "            del transformer[\"h\"][l].ln_2.weight\n",
    "            transformer[\"h\"][l].ln_2.weight = nn.Parameter(l2w[l])\n",
    "\n",
    "            transformer[\"h\"][l].attn.c_attn.weight = nn.Parameter(c_attn_weight[l])\n",
    "            assert transformer[\"h\"][l].attn.c_proj.weight.shape == c_attn_proj[l].shape\n",
    "            transformer[\"h\"][l].attn.c_proj.weight = nn.Parameter(c_attn_proj[l])\n",
    "\n",
    "            del transformer[\"h\"][l].mlp.c_fc2.weight\n",
    "            transformer[\"h\"][l].mlp.c_fc2.weight = nn.Parameter(mlp_w1[l])\n",
    "\n",
    "            del transformer[\"h\"][l].mlp.c_fc.weight\n",
    "            transformer[\"h\"][l].mlp.c_fc.weight = nn.Parameter(mlp_w1[l].detach())\n",
    "\n",
    "            del transformer[\"h\"][l].mlp.c_proj.weight\n",
    "            transformer[\"h\"][l].mlp.c_proj.weight = nn.Parameter(mlp_w2[l].detach())\n",
    "\n",
    "        return init_mymodel_from_LLama(config, llama, False), config\n",
    "        \n",
    "def load_state(state_path, V: int):\n",
    "    with open(state_path, \"rb\") as f:\n",
    "        header = np.frombuffer(buffer=f.read(256 * 4), dtype=np.int32)\n",
    "        B, T = header[2:4]\n",
    "        inputs = torch.from_numpy(\n",
    "            np.frombuffer(f.read(B * T * 4), dtype=np.int32),\n",
    "        ).view(B, T)\n",
    "        # inputs = inputs.to(torch.long)\n",
    "        targets = (\n",
    "            torch.from_numpy(np.frombuffer(f.read(B * T * 4), dtype=np.int32))\n",
    "            .view(B, T)\n",
    "            .to(torch.long)\n",
    "        )\n",
    "        logits = torch.from_numpy(\n",
    "            np.frombuffer(f.read(B * T * V * 4), dtype=np.float32)\n",
    "        ).view(B, T, V)\n",
    "        loss = torch.from_numpy(np.frombuffer(f.read(4), dtype=np.float32))\n",
    "        print(B, T, loss)\n",
    "        return inputs, targets, logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ea3a8ed1-104c-452c-89d6-4eab7cdfe6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_model_path = \"gpt2_124M.bin\"           # the gpt2 model params\n",
    "state_path = \"gpt2_124M_debug_state.bin\"   # the inputs, and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2c076c18-7c76-47e4-bb97-c49f1d278b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download the models\n",
    "import os\n",
    "if not os.path.exists(bin_model_path):\n",
    "    !curl -L -o {bin_model_path} \"https://huggingface.co/datasets/karpathy/llmc-starter-pack/resolve/main/gpt2_124M.bin\"\n",
    "if not os.path.exists(state_path):\n",
    "    !curl  -L -o {state_path} \"https://huggingface.co/datasets/karpathy/llmc-starter-pack/resolve/main/gpt2_124M_debug_state.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b5b07050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 50257 12 12 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaConfig(version='3.1', block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_kv_head=12, n_embd=768, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=5, B=4)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mymodel, llama_model), config = load_model(bin_model_path)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "913ad26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 64 tensor([5.2700086])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaConfig(version='3.1', block_size=1024, vocab_size=50257, n_layer=12, n_head=12, n_kv_head=12, n_embd=768, ffn_dim_multiplier=1.3, multiple_of=32, norm_eps=1e-05, rope_theta=500000.0, use_scaled_rope=False, max_gen_batch_size=4, use_kv=True, flash=False, T=64, B=4)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets, logits, loss = load_state(state_path=state_path, V=config.vocab_size)\n",
    "config.B, config.T = inputs.shape\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "50e9b213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2998238)\n"
     ]
    }
   ],
   "source": [
    "mylogits, myloss = mymodel.forward(inp=inputs, targets=targets)\n",
    "print(myloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "89e8429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2998266, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "llama_logits, llama_loss = llama_model.forward(\n",
    "    inputs, targets=targets, return_logits=True\n",
    ")\n",
    "print(llama_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9c8ac977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(llama_logits, mylogits, atol=1e-4))\n",
    "print(torch.allclose(llama_loss, myloss, atol=1e-4, rtol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1cccff-1334-4fbd-807e-e8b7c2be85aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
